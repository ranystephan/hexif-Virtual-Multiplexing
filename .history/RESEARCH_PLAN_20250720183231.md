# H&E to Spatial Transcriptomics Prediction: Research Plan

## Project Overview

**Goal**: Develop models to predict molecular/spatial information from Orion multiplexed images using only H&E histopathology images.

**Data**: 273 paired H&E-Orion image cores from TA118 dataset with 16 macrophage-related protein markers.

**Timeline**: 12 weeks, 3 phases

---

## Phase 1: Foundation & Baseline (Weeks 1-4) ðŸ—ï¸

### Week 1: Data Pipeline & Initial Analysis

#### [ ] Task 1.1: Data Exploration & Biological Limits Assessment
- **Priority**: CRITICAL - Do this first to set realistic expectations
- **Action Items**:
  - [ ] Analyze correlation between H&E morphological features and each Orion marker
  - [ ] Identify "easy" vs "hard" markers using simple regression baselines
  - [ ] Establish theoretical upper bounds per marker
  - [ ] Document biological rationale for predictable vs unpredictable markers

```python
# Predictability Analysis Framework
def assess_marker_predictability(he_features, orion_expressions):
    """Estimate upper bounds for each marker using simple models"""
    predictability_scores = {}
    
    for marker in orion_markers:
        # Simple linear regression baseline
        lr_score = cross_validate_linear_regression(he_features, orion_expressions[marker])
        
        # Random forest to capture non-linearity
        rf_score = cross_validate_random_forest(he_features, orion_expressions[marker])
        
        predictability_scores[marker] = {
            'linear_r2': lr_score,
            'rf_r2': rf_score,
            'difficulty_rank': rank_marker_difficulty(rf_score)
        }
    
    return predictability_scores
```

**Success Criteria**: 
- Clear ranking of marker predictability
- Identification of 3-5 "focus markers" for initial model development
- Documented biological justification for expectations

#### [X] Task 1.2: VALIS-Based Registration Pipeline âœ…
- **Priority**: HIGH - Address registration bottleneck proactively
- **Approach**: VALIS (Virtual Alignment of pathoLogy Image Series) - Professional-grade registration
- **Action Items**:
  - [X] Implement VALIS-based registration for whole H&E-Orion pairs
  - [X] Develop comprehensive registration quality assessment (SSIM, NCC, MI)
  - [X] Create robust data loading pipeline for registered pairs
  - [X] Handle registration failures gracefully with quality thresholds
  - [X] Set up batch processing for full dataset (273 pairs)

```python
class VALISRegistrationPipeline:
    def __init__(self, output_dir, registration_params=None):
        self.params = registration_params or {
            'max_processed_image_dim_px': 1024,
            'max_non_rigid_registration_dim_px': 1500,
            'image_type': 'multi',
            'imgs_ordered': True,
            'align_to_reference': True,
        }
    
    def register_single_pair(self, he_path, orion_path, core_id):
        # VALIS registration with H&E as reference
        # Quality assessment with SSIM, NCC, MI
        # Save only successful registrations
```

**Success Criteria**: âœ…
- VALIS registration pipeline implemented in `02_registration_robust_pipeline.ipynb`
- Comprehensive quality metrics (SSIM, NCC, MI) with success thresholds
- Robust dataset class for training with quality filtering
- Batch processing capability for full dataset
- **Ready for whole-image Orion prediction model development**

### Week 2: ROSIE Baseline Implementation

#### [ ] Task 2.1: Adapt ROSIE Architecture
- **Priority**: CRITICAL - Our strongest baseline
- **Action Items**:
  - [ ] Modify ROSIE's ConvNeXt backbone for 16 Orion markers
  - [ ] Implement patch-to-cell aggregation for expression prediction
  - [ ] Add cell segmentation map prediction as secondary task
  - [ ] Create training loop with proper validation

**Architecture Details**:
```python
class AdaptedROSIE(nn.Module):
    def __init__(self, num_markers=16):
        super().__init__()
        self.backbone = models.convnext_small(weights='IMAGENET1K_V1')
        
        # Multi-task heads
        self.expression_head = nn.Linear(768, num_markers)
        self.segmentation_head = nn.Conv2d(768, 1, 1)  # Cell probability map
        
    def forward(self, x):
        features = self.backbone.features(x)
        
        # Expression prediction
        pooled_features = F.adaptive_avg_pool2d(features, 1).flatten(1)
        expressions = self.expression_head(pooled_features)
        
        # Segmentation prediction
        seg_map = self.segmentation_head(features)
        
        return expressions, seg_map
```

**Success Criteria**:
- Model trains without errors
- Achieves >0.3 average correlation on focus markers
- Segmentation maps show reasonable cell boundaries

#### [ ] Task 2.2: Simple CNN Baseline
- **Priority**: MEDIUM - Quick additional baseline
- **Action Items**:
  - [ ] Implement ResNet50 â†’ 16 expression values
  - [ ] Train with same data pipeline
  - [ ] Compare against ROSIE performance

**Success Criteria**:
- Baseline correlation >0.2 average on focus markers
- Training time <24 hours on available hardware

### Week 3-4: Evaluation Framework & SpaceC Integration

#### [ ] Task 3.1: Comprehensive Evaluation System
- **Action Items**:
  - [ ] Implement multi-level metrics (image, cell, spatial, biological)
  - [ ] Create automated evaluation pipeline
  - [ ] Set up experiment tracking (wandb/mlflow)
  - [ ] Design statistical significance testing

```python
class ComprehensiveEvaluator:
    def __init__(self, spacec_processor=None):
        self.spacec = spacec_processor
        
    def evaluate_model(self, model, test_loader):
        results = {
            'expression_metrics': self.compute_expression_metrics(),
            'spatial_metrics': self.compute_spatial_metrics(),
            'biological_metrics': self.validate_known_biology()
        }
        return results
        
    def compute_expression_metrics(self, predictions, ground_truth):
        return {
            'pearson_correlation': compute_correlations(predictions, ground_truth),
            'r2_scores': compute_r2_scores(predictions, ground_truth),
            'mae_per_marker': compute_mae_per_marker(predictions, ground_truth)
        }
```

#### [ ] Task 3.2: SpaceC Integration (Budget: 1 week max)
- **Priority**: MEDIUM - Important but time-boxed
- **Action Items**:
  - [ ] Set up SpaceC environment and test on sample data
  - [ ] Create prediction â†’ SpaceC analysis pipeline
  - [ ] Implement spatial validation metrics
  - [ ] **Fallback plan**: If SpaceC integration is problematic, use simpler spatial metrics

**Success Criteria**:
- SpaceC processes predictions without errors
- Can compare spatial patterns between predicted and ground truth
- Clear fallback if integration fails

---

## Phase 2: Model Development (Weeks 5-8) ðŸš€

### Week 5-6: Multi-Task ConvNeXt Enhancement

#### [ ] Task 4.1: Enhanced Multi-Task Architecture
- **Priority**: HIGH - Build on ROSIE success
- **Action Items**:
  - [ ] Add attention modules for interpretability
  - [ ] Implement multi-scale feature extraction
  - [ ] Add auxiliary losses (cell counting, tissue type classification)
  - [ ] Hyperparameter optimization

```python
class EnhancedMultiTaskModel(nn.Module):
    def __init__(self, num_markers=16):
        super().__init__()
        self.backbone = ConvNeXtWithAttention()
        self.multi_scale_fusion = MultiScaleFusion()
        
        # Multiple prediction heads
        self.expression_head = ExpressionPredictor(num_markers)
        self.segmentation_head = SegmentationPredictor()
        self.attention_head = AttentionPredictor()  # For interpretability
        
    def forward(self, x):
        # Multi-scale feature extraction
        features = self.multi_scale_fusion(self.backbone(x))
        
        # Multi-task predictions
        expressions = self.expression_head(features)
        segmentation = self.segmentation_head(features)
        attention_maps = self.attention_head(features)
        
        return {
            'expressions': expressions,
            'segmentation': segmentation, 
            'attention': attention_maps
        }
```

#### [ ] Task 4.2: Registration-Robust Training Strategies
- **Action Items**:
  - [ ] Implement patch-based contrastive learning
  - [ ] Test weakly supervised approaches
  - [ ] Add noise-robust loss functions

### Week 7: Vision Transformer (Conditional Implementation)

#### [ ] Task 5.1: ViT Implementation (Only if ConvNeXt hits ceiling)
- **Priority**: CONDITIONAL - Only proceed if baseline performance plateaus
- **Decision Criteria**: If enhanced ConvNeXt achieves <0.4 average correlation
- **Action Items**:
  - [ ] Implement ViT with spatial position encoding
  - [ ] Add cross-attention between patches
  - [ ] Compare computational costs vs performance gains

**Resource Check**: Ensure sufficient GPU memory before proceeding

### Week 8: Alternative Approaches (Optional)

#### [ ] Task 6.1: Contrastive Learning Approach
- **Priority**: RESEARCH INTEREST - Novel approach suggested by reviewer
- **Action Items**:
  - [ ] Implement multimodal embedding model
  - [ ] Map H&E and Orion to shared latent space
  - [ ] Test feature disentanglement capabilities

```python
class MultimodalContrastiveModel(nn.Module):
    def __init__(self, embedding_dim=512):
        super().__init__()
        self.he_encoder = ConvNeXtEncoder()
        self.orion_encoder = ConvNeXtEncoder()
        self.projection_head = nn.Linear(768, embedding_dim)
        
    def forward(self, he_images, orion_images):
        he_embeddings = self.projection_head(self.he_encoder(he_images))
        orion_embeddings = self.projection_head(self.orion_encoder(orion_images))
        
        # Contrastive loss between paired embeddings
        return he_embeddings, orion_embeddings
```

---

## Phase 3: Analysis & Production (Weeks 9-12) ðŸ”¬

### Week 9-10: Interpretability & Ablation Studies

#### [ ] Task 7.1: Attention Map Analysis
- **Action Items**:
  - [ ] Visualize attention maps for each marker
  - [ ] Correlate attention patterns with known histology features
  - [ ] Create interpretability dashboard

#### [ ] Task 7.2: Occlusion Studies
- **Action Items**:
  - [ ] Systematically mask H&E regions
  - [ ] Measure prediction degradation per marker
  - [ ] Map spatial importance across tissue types

#### [ ] Task 7.3: Marker-Specific Analysis
- **Action Items**:
  - [ ] Analyze prediction quality per marker
  - [ ] Correlate with biological prior knowledge
  - [ ] Document biological insights

### Week 11-12: Production Pipeline & Documentation

#### [ ] Task 8.1: Production Pipeline
- **Action Items**:
  - [ ] Containerize inference pipeline (Docker)
  - [ ] Optimize for inference speed
  - [ ] Create REST API for predictions
  - [ ] Add model versioning and logging

#### [ ] Task 8.2: Comprehensive Documentation
- **Action Items**:
  - [ ] Technical documentation for reproducibility
  - [ ] Model cards for each trained model
  - [ ] Biological interpretation guide
  - [ ] Performance benchmarking report

---

## Resource Planning & Risk Mitigation ðŸ›¡ï¸

### Computational Resources

**Estimated Requirements**:
- **GPU Memory**: 16GB+ for training (24GB preferred)
- **Storage**: 500GB for data, models, and results
- **Training Time**: ~2-3 days per major model iteration

**Fallback Plans**:
- If GPU memory insufficient: Reduce batch size, use gradient accumulation
- If training too slow: Focus on smaller models, reduce data augmentation

### Key Decision Points

**Week 2 Decision**: ROSIE baseline performance
- If >0.4 correlation: Proceed with enhancement
- If 0.2-0.4 correlation: Focus on data quality improvements
- If <0.2 correlation: Reassess problem feasibility

**Week 6 Decision**: Architecture advancement
- If enhanced ConvNeXt shows improvement: Continue optimization
- If plateau reached: Consider ViT implementation
- If compute limited: Focus on interpretability and production

**Week 8 Decision**: Final model selection
- Choose best performing architecture for production
- Begin comprehensive evaluation and documentation

### Success Metrics

**Minimum Viable Product**:
- Average correlation >0.3 across focus markers
- Spatial patterns preserved in >70% of cases
- Production-ready inference pipeline

**Stretch Goals**:
- Average correlation >0.5 across all markers
- Novel biological insights from attention analysis
- Publishable research contribution

---

## Experiment Tracking Template

### Weekly Progress Template

```markdown
## Week X Progress Report

### Completed Tasks
- [ ] Task description
- [ ] Results: [metrics/outcomes]
- [ ] Challenges: [issues encountered]

### Key Metrics
- Marker correlations: [list per marker]
- Computational time: [training/inference]
- Resource usage: [GPU memory, storage]

### Decisions Made
- [Decision] based on [criteria/results]
- Next week priorities adjusted: [changes]

### Blockers & Risks
- [Current blockers]
- [Mitigation strategies]

### Next Week Priorities
1. [High priority task]
2. [Medium priority task]
3. [Low priority task]
```

---

## Final Notes

**Philosophy**: Start simple, build systematically, validate continuously.

**Key Principles**:
1. **Biological validation first** - Technical metrics must translate to biological relevance
2. **Iterative improvement** - Small, validated steps rather than large jumps
3. **Resource awareness** - Stay within computational and time constraints
4. **Reproducibility** - Document everything for future research

**Contact Points**: Plan weekly check-ins to assess progress and adjust priorities based on results. 