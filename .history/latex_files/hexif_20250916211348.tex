\documentclass[12pt,twoside]{article}
\usepackage[margin=1in]{geometry}
\usepackage[switch]{lineno}
\linenumbers
\usepackage{setspace}
\onehalfspacing
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}

\title{HExIF: End-to-End Deep Learning Framework for Virtual Multiplex Immunofluorescence from H\&E Slides}
\author{Rany Stephan\thanks{Institute for Computational and Mathematical Engineering, Stanford University. \texttt{ranycs@stanford.edu}} \and
        Andrew Gentles\thanks{Department of Biomedical Data-Science, Stanford University.}}
\date{}

\begin{document}
\maketitle

\begin{abstract}
\noindent
High-plex immunofluorescence platforms such as Orion or CODEX enable simultaneous measurement of dozens of biomarkers in a single tissue section, but they require specialized instrumentation, extensive imaging time, and high reagent cost. Hematoxylin and eosin (H\&E) staining is ubiquitous in pathology workflows but lacks molecular specificity. We present HExIF, a modular pipeline that aligns H\&E and immunofluorescence whole-slide images, extracts matched tissue cores, and prepares data for downstream machine learning models that will predict multiplex fluorescence channels directly from H\&E. To date, we have successfully registered an extensive dataset of paired cores from H\&E and Orion slides of a tissue microarray, validated alignment by edge overlays and local zoom-in comparisons, and automated tissue core extraction using SPACEc. These steps yield a high-quality dataset of paired RGB patches for model training. Ongoing work focuses on implementing and evaluating conditional generative and segmentation-aware networks, including pix2pix and multi-task UNet architectures, to synthesize fluorescence intensity maps and cell segmentation masks. We describe here the methodology, preliminary alignment results, and the planned modeling and validation strategies.
\end{abstract}

\section{Introduction}
\label{sec:intro}
\noindent
The ability to profile multiple protein markers in situ at single‐cell resolution has transformed spatial biology, with high‐plex immunofluorescence technologies such as CODEX and Orion providing up to 20–40 channels of molecular information per tissue section. Despite their power, these methods remain inaccessible to many laboratories due to cost, specialized instrumentation, and complex workflows. Conversely, H\&E staining is performed in nearly every pathology laboratory worldwide, offering widespread availability but limited to morphological contrasts.

\noindent
Recent advances in image‐to‐image translation using deep learning have demonstrated the feasibility of predicting fluorescence modalities from brightfield histology. However, most demonstrations have focused on a small number of markers or lacked rigorous evaluation against high‐plex ground truth. HExIF aims to bridge this gap by constructing a fully automated pipeline that (1) co‐registers H\&E and immunofluorescence whole‐slide images using feature‐based projective transforms and RANSAC, (2) extracts matched tissue cores via SPACEc’s downscaling and segmentation tools, (3) performs quality control through Canny edge overlays and localized difference heatmaps, and (4) feeds paired cores into deep generative and segmentation models to predict multi-channel fluorescence and cell masks directly from H\&E.

\noindent
In this study, we first describe our registration protocol, which achieves sub‐pixel alignment as verified by overlaying H\&E and transformed Orion thumbnails and inspecting multiple regions of interest. We then detail the use of SPACEc for unbiased extraction of 273 core patches at full resolution, each paired between H\&E and Orion. Preliminary analysis confirms consistent tissue coverage and minimal cropping artifacts. The dataset is now prepared for machine learning experiments.

\noindent
Future work will implement two complementary modeling approaches. The first uses a conditional adversarial network (pix2pix) to translate H\&E RGB and auxiliary segmentation masks into 16 fluorescence channels, optimizing L1 and perceptual losses. The second employs a dual‐head UNet that predicts both fluorescence intensities and cell segmentation in a multi‐task framework, trained on paired cores and segmentation masks derived from SPACEc’s cell segmentation on Orion data. We will evaluate models quantitatively via per‐channel Pearson correlation and Dice scores for segmentation, and qualitatively through downstream cell phenotyping and clustering analyses. This framework has the potential to democratize high‐plex spatial profiling by enabling \emph{in silico} multiplex reconstruction from routine H\&E slides.

\section{Data Preparation and Quality Control}
To ensure the integrity of our paired H\&E and Orion datasets, we implemented a rigorous, multi‐stage data preparation and quality control (QC) pipeline. First, each H\&E core was read as an RGB image and converted to its hematoxylin component using color deconvolution (\texttt{rgb2hed}), while the corresponding Orion tile was loaded as a 20‐channel fluorescence stack. Because minor misalignments—even at the subpixel level—can induce spurious correlations between morphological features and marker expression, we performed a quantitative registration check on every core. We extracted edge maps from the normalized hematoxylin channel and the DAPI fluorescence channel using Canny edge detection, and then computed the optimal translation vector via phase‐cross‐correlation with tenfold upsampling. Applying this translation to the DAPI map yielded subpixel‐aligned edge overlays, in which pixel‐wise normalized cross‐correlation (NCC) reliably quantified registration quality. To tolerate slight boundary mismatches, the hematoxylin edges were dilated by a one‐pixel disk before NCC scoring; cores with NCC below 0.90 were flagged for manual review.

Following spatial QC, each Orion channel was independently normalized to mitigate differences in dynamic range and background noise. We clipped intensities at the 1st and 99th percentiles to suppress outliers, applied a \(\log(1+x)\) transform to stabilize variance, and z‐scored the result so that all twenty channels shared zero mean and unit variance. These operations were recorded in per‐core JSON reports, preserving the exact percentiles, means, and standard deviations for downstream reproducibility and potential inverse transformation.

With both alignment and intensity normalization complete, we extracted paired 256 × 256 px patches at a target resolution of 0.5 µm/px, using 50 \% overlap to maximize tissue coverage. To avoid sampling empty background, patches with median DAPI intensity below a threshold of 0.05 (normalized units) were discarded. Each retained patch yielded three artifacts: the raw H\&E subimage, the 20‐channel normalized Orion cube, and a binary IBA1 mask generated via adaptive thresholding and morphological opening. The IBA1 mask served both as a biological guide for later macrophage‐specific analyses and as the basis for computing per‐channel sparsity statistics, which informed weighted patch sampling during model training. Finally, to prevent data leakage across patient samples, we performed a five‐fold core‐level split using K‐fold shuffling of core identifiers, ensuring that all patches from any given core remained within the same fold. This comprehensive pipeline produced a high‐quality, reproducible dataset of well‐aligned, normalized, and biologically annotated image patches, ready for subsequent model development.


\subsection{Hybrid Training Pipeline}
We implemented a unified training script, \texttt{step2\_train.py}, to systematically evaluate two model families under identical conditions.  The script accepts command-line arguments to select between (1) a Phase-1 U-Net baseline with four Orion channels (Pan-CK, IBA1, CD163, CD3e) and (2) a Phase-2 ResNet-34 + ASPP encoder–decoder network.  In both cases, we perform on-the-fly H\&E augmentation (random flips, colour jitter, and random cropping to a fixed patch size) and train with a composite loss that combines mean‐absolute‐error on the fluorescence regression head and a Dice loss on the IBA1 segmentation head.  Training uses the Adam optimizer with cosine-annealing of the learning rate, and models are checkpointed automatically when the validation loss improves, with early stopping after a user-defined patience.

\subsection{Five-Fold Cross-Validation}
To obtain robust performance estimates, we split the 273 paired H\&E–Orion cores at the core level into five non-overlapping folds.  By passing \texttt{--fold -1} the script loops over folds 0–4, using each in turn as the validation set and training on the remaining four folds.  For each fold we record per-epoch metrics—validation loss, L1 regression error, Pearson correlation on predicted versus true fluorescence, and Dice score for IBA1 segmentation—in a \texttt{log.csv} file.  After all five folds have run, these logs can be aggregated to compute mean and standard deviation across folds, ensuring that our reported results reflect both central tendency and variability.

\subsection{Phase-2 ResNet-ASPP Network}
Building on the Phase-1 baseline, we implemented an advanced Phase-2 architecture that replaces the scratch-trained U-Net encoder with an ImageNet-pretrained ResNet-34 backbone, followed by an Atrous Spatial Pyramid Pooling (ASPP) module to capture multi-scale context.  The ASPP output is passed through a lightweight decoder with transposed convolutions to restore spatial resolution, and finally through separate convolutional heads for fluorescence regression and IBA1 segmentation.  All other aspects of training—augmentation, loss functions, optimizer schedule, early stopping, and cross-validation—remain identical, allowing a direct comparison of architectural impact on virtual multiplex immunofluorescence performance.


  
\bibliographystyle{plain}
\begin{thebibliography}{9}
\bibitem{Goltsev2018}
Goltsev, Y., Samusik, N., Kennedy-Darling, J., et al. Deep Profiling of Mouse Splenic Architecture with CODEX Multiplexed Imaging. \emph{Cell}, 174(4):968–981, 2018.

\bibitem{Angel2017}
Angel, M., et al. Orion\texttrademark: A high‐plex immunofluorescence platform for spatial single‐cell analysis. \emph{Nature Methods}, 14:209–216, 2017.

\bibitem{Isola2017}
Isola, P., Zhu, J.-Y., Zhou, T., Efros, A.A. Image-to-Image Translation with Conditional Adversarial Networks. \emph{CVPR}, 2017.

\end{thebibliography}

\end{document}
