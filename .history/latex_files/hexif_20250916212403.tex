\documentclass[12pt,twoside]{article}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\onehalfspacing
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{url}

\title{HExIF: An End-to-End Deep Learning Framework for Virtual Multiplex Immunofluorescence from H\&E Slides}
\author{Rany Stephan\thanks{Institute for Computational and Mathematical Engineering, Stanford University. \texttt{ranycs@stanford.edu}} \and
        Andrew Gentles\thanks{Department of Biomedical Data-Science, Stanford University.}}
\date{}

\begin{document}
\maketitle

\begin{abstract}
\noindent
High-plex immunofluorescence technologies provide deep, spatially-resolved molecular data but remain costly and complex. In contrast, Hematoxylin and Eosin (H\&E) staining is a ubiquitous, low-cost histological method limited to morphological assessment. To bridge this gap, we present HExIF, an automated deep learning pipeline to generate virtual multiplex immunofluorescence images directly from standard H\&E slides. Our framework begins by co-registering whole-slide images of paired H\&E and 20-channel Orion immunofluorescence slides from a tissue microarray using the \texttt{valis} registration library. We then employ a custom image processing workflow to automatically detect and extract over 270 matched, full-resolution tissue cores. From these cores, we generate a large dataset of paired image patches for training a U-Net-based deep learning model. The model is trained to predict all 20 fluorescence channels from a single H\&E input patch using a specialized, spatially-aware loss function designed to preserve fine-grained details and prevent mode collapse. This paper details our complete methodology, from slide registration and automated data curation to the architecture and training of the predictive model, establishing a robust framework for large-scale \emph{in silico} multiplex imaging.
\end{abstract}

\section{Introduction}
\label{sec:intro}
\noindent
The study of complex cellular interactions within their native tissue context, known as spatial biology, has been revolutionized by high-plex imaging technologies. Platforms like CODEX \cite{Goltsev2018} and Orion \cite{Angel2017} enable the simultaneous visualization of dozens of protein markers on a single tissue section, providing unprecedented insight into tissue microenvironments, particularly in fields like oncology and immunology. However, the specialized instrumentation, high reagent costs, and complex experimental workflows associated with these methods limit their widespread adoption. In stark contrast, Hematoxylin and Eosin (H\&E) staining is the cornerstone of histopathology, performed routinely in clinical and research settings worldwide. While inexpensive and highly standardized, H\&E provides only morphological information and lacks molecular specificity.

\noindent
Recent advances in deep learning, particularly in image-to-image translation, have opened the possibility of "virtual staining"---computationally predicting molecular or special stains from standard H\&E images \cite{Isola2017}. This approach promises to unlock a wealth of molecular information from vast archives of existing H\&E slides, democratizing high-plex spatial profiling. HExIF (H\&E to virtual Immunofluorescence) is an end-to-end framework designed to realize this promise by learning to predict 20-channel Orion fluorescence images directly from their paired H\&E counterparts.

\noindent
Our pipeline addresses several key challenges in this domain. First, it implements a fully automated workflow for the precise, pixel-level registration of gigapixel whole-slide images. Second, it automates the detection and extraction of corresponding tissue cores from tissue microarrays (TMAs), generating a large-scale, curated dataset for model training. Finally, it employs a U-Net-based architecture trained with a custom, spatially-aware loss function tailored to the unique characteristics of immunofluorescence data. This paper describes the complete HExIF methodology, covering the data preparation pipeline and the deep learning framework in detail.
