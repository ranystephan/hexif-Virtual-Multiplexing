{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TMA core pairing pipeline\n",
        "\n",
        "This notebook implements a pipeline to detect tissue micro‑array (TMA) cores in two whole‑slide images, estimate a global alignment between the slides and then pair individual cores.  The final output is a set of paired patches (H&E and Orion) and quick‑look overlays that can be used for downstream registration (e.g. with VALIS) and model training.  The approach is modular and designed to be robust to differences in image cropping and modality.  The pipeline consists of four main stages:\n",
        "\n",
        "1. **Thumbnail extraction and core detection** – Load a low‑resolution thumbnail from each OME‑TIFF and detect circular TMA cores using adaptive thresholding, morphological opening/closing and contour filtering.  Detection is performed on grayscale thumbnails; an optional `brightfield` flag adjusts the thresholding behaviour for fluorescent versus brightfield slides.\n",
        "2. **Global alignment of core centroids** – Obtain the approximate transformation that brings the Orion core centroids into the H&E coordinate system.  By default the notebook uses a Procrustes alignment (scale + rotation + translation) via singular value decomposition (SVD).  If the [pycpd](https://pypi.org/project/pycpd/) package is installed, you can switch to coherent point drift (CPD) for potentially more robust point‑set registration【549576595033251†L48-L67】.\n",
        "3. **Core pairing** – Compute a distance matrix between the transformed Orion centroids and the H&E centroids, and solve the assignment problem with the Hungarian algorithm (`scipy.optimize.linear_sum_assignment`).  Pairs whose distances exceed a threshold (multiples of the median inter‑core spacing) are rejected.\n",
        "4. **Patch extraction and quality control** – For each paired core, extract a square region around the centre from both full‑resolution slides and save it as a compressed TIFF.  A quick‑look overlay (H&E in red, Orion in green) is saved as a PNG for manual inspection.  A CSV file summarises each pair and its file paths.\n",
        "\n",
        "This pipeline assumes that both the H&E and Orion images are OME‑TIFF files with a pyramidal structure.  It relies on the [`tifffile`](https://pypi.org/project/tifffile/) library to read individual pyramid levels and sub‑regions.  If your environment lacks `tifffile` you will need to install it or adapt the `load_thumbnail` and `extract_region` functions to use another WSI reader.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import math\n",
        "import numpy as np\n",
        "import cv2\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "# Try importing tifffile; fall back to PIL if unavailable\n",
        "try:\n",
        "    import tifffile\n",
        "    TIFF_AVAILABLE = True\n",
        "except ImportError:\n",
        "    from PIL import Image\n",
        "    TIFF_AVAILABLE = False\n",
        "\n",
        "# Try importing pycpd for coherent point drift; otherwise we'll use a simple Procrustes alignment\n",
        "try:\n",
        "    from pycpd import AffineRegistration\n",
        "    CPD_AVAILABLE = True\n",
        "except Exception:\n",
        "    CPD_AVAILABLE = False\n",
        "\n",
        "from scipy.optimize import linear_sum_assignment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_thumbnail(slide_path: str, thumb_size: int = 4096, use_channel0: bool = False) -> Tuple[np.ndarray, Tuple[int, int]]:\n",
        "    \"\"\"\n",
        "    Load a low‑resolution thumbnail from an OME‑TIFF slide.\n",
        "    If `tifffile` is available, the lowest pyramid level is returned.\n",
        "    Otherwise, the image is opened via Pillow and resized down.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    slide_path : str\n",
        "        Path to the OME‑TIFF.\n",
        "    thumb_size : int\n",
        "        Target size of the largest dimension of the thumbnail.\n",
        "    use_channel0 : bool\n",
        "        For fluorescence images that may have multiple channels, force use of the first channel.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    thumb : np.ndarray\n",
        "        Grayscale thumbnail image.\n",
        "    full_shape : Tuple[int, int]\n",
        "        (height, width) of the full resolution image.\n",
        "    \"\"\"\n",
        "    if TIFF_AVAILABLE:\n",
        "        # Read via tifffile; this handles OME pyramids robustly\n",
        "        with tifffile.TiffFile(slide_path) as tif:\n",
        "            series = tif.series[0]\n",
        "            full_shape = series.shape[-2:]  # (height, width)\n",
        "            # Use the lowest resolution level\n",
        "            level = series.levels[-1]\n",
        "            arr = level.asarray()\n",
        "            # arr might be (samples, h, w) or (h, w, samples)\n",
        "            if arr.ndim == 3:\n",
        "                # (samples, y, x) ordering (OME)\n",
        "                if arr.shape[0] < arr.shape[-1]:\n",
        "                    arr = arr[0] if use_channel0 else arr[0]\n",
        "                else:\n",
        "                    arr = arr[..., 0] if use_channel0 else cv2.cvtColor(arr, cv2.COLOR_RGB2GRAY)\n",
        "            thumb = arr.astype(np.uint8)\n",
        "        # Resize up if the smallest dimension is too small\n",
        "        h, w = thumb.shape\n",
        "        if max(h, w) < thumb_size:\n",
        "            scale = thumb_size / max(h, w)\n",
        "            thumb = cv2.resize(thumb, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_CUBIC)\n",
        "    else:\n",
        "        # Fallback: open via PIL and downsample\n",
        "        img = Image.open(slide_path)\n",
        "        img_arr = np.array(img)\n",
        "        # Convert to grayscale\n",
        "        if img_arr.ndim == 3:\n",
        "            img_gray = cv2.cvtColor(img_arr, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            img_gray = img_arr\n",
        "        full_shape = img_gray.shape\n",
        "        h, w = img_gray.shape\n",
        "        scale = thumb_size / max(h, w) if max(h, w) > thumb_size else 1.0\n",
        "        thumb = cv2.resize(img_gray, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_AREA)\n",
        "    return thumb, full_shape\n",
        "\n",
        "def detect_cores(\n",
        "    image: np.ndarray,\n",
        "    brightfield: bool = True,\n",
        "    min_area: int = 100,\n",
        "    circularity_thresh: float = 0.1\n",
        ") -> List[Dict[str, object]]:\n",
        "    \"\"\"\n",
        "    Detect approximate circular cores in a grayscale thumbnail.\n",
        "\n",
        "    Uses adaptive thresholding followed by morphological opening and closing to generate a binary mask.\n",
        "    Contours are extracted and filtered by area and circularity.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    image : np.ndarray\n",
        "        Grayscale thumbnail.\n",
        "    brightfield : bool\n",
        "        If True, uses inverse thresholding appropriate for brightfield images.\n",
        "        If False, uses thresholding for fluorescence images.\n",
        "    min_area : int\n",
        "        Minimum contour area (in pixel^2 of the thumbnail) to consider a core.\n",
        "    circularity_thresh : float\n",
        "        Minimum circularity metric (4π * area / perimeter^2) to accept a contour.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    cores : List[Dict]\n",
        "        List of detected cores, each with keys 'center' (tuple of ints) and 'area'.\n",
        "    \"\"\"\n",
        "    # Normalize intensity and blur\n",
        "    img = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "    blur = cv2.GaussianBlur(img, (5, 5), 0)\n",
        "    # Apply CLAHE for fluorescent images to enhance contrast\n",
        "    if not brightfield:\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "        blur = clahe.apply(blur)\n",
        "    # Adaptive thresholding parameters\n",
        "    bs = max(31, int(min(blur.shape) // 50) * 2 + 1)\n",
        "    thresh_type = cv2.THRESH_BINARY_INV if brightfield else cv2.THRESH_BINARY\n",
        "    binary = cv2.adaptiveThreshold(\n",
        "        blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, thresh_type, bs, 2\n",
        "    )\n",
        "    # Morphological clean up\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
        "    binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
        "    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cores = []\n",
        "    for cnt in contours:\n",
        "        area = cv2.contourArea(cnt)\n",
        "        if area < min_area:\n",
        "            continue\n",
        "        M = cv2.moments(cnt)\n",
        "        if M['m00'] == 0:\n",
        "            continue\n",
        "        cx = int(M['m10'] / M['m00'])\n",
        "        cy = int(M['m01'] / M['m00'])\n",
        "        perimeter = cv2.arcLength(cnt, True)\n",
        "        circularity = 4 * math.pi * area / (perimeter * perimeter + 1e-6)\n",
        "        if circularity < circularity_thresh:\n",
        "            continue\n",
        "        cores.append({'center': (cx, cy), 'area': area})\n",
        "    return cores\n",
        "\n",
        "def scale_centres(cores: List[Dict[str, object]], thumb_shape: Tuple[int, int], full_shape: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
        "    \"\"\"Scale thumbnail coordinates back to full resolution.\"\"\"\n",
        "    h_t, w_t = thumb_shape\n",
        "    h_f, w_f = full_shape\n",
        "    sx = w_f / w_t\n",
        "    sy = h_f / h_t\n",
        "    return [(int(x * sx), int(y * sy)) for (x, y) in [c['center'] for c in cores]]\n",
        "\n",
        "def procrustes_align(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Align the moving point set A to the fixed point set B using a similarity transform (scale + rotation + translation).\n",
        "    Returns the transformed version of A.\n",
        "    \"\"\"\n",
        "    A = A.astype(float)\n",
        "    B = B.astype(float)\n",


        "
        "    # Handle different sized point sets by using a subset approach\n",
        "    if len(A) != len(B):\n",
        "        # Use the smaller set size for initial alignment\n",
        "        min_size = min(len(A), len(B))\n",
        "        if min_size < 3:\n",
        "            # Not enough points for reliable alignment, use simple translation\n",
        "            centroid_A = A.mean(axis=0)\n",
        "            centroid_B = B.mean(axis=0)\n",
        "            return A + (centroid_B - centroid_A)\n",
        "        \n",
        "        # For different sized sets, we'll compute the transformation using centroids and scale\n",
        "        # This is a simplified approach that works when point sets have different sizes\n",
        "        centroid_A = A.mean(axis=0)\n",
        "        centroid_B = B.mean(axis=0)\n",
        "        \n",
        "        # Estimate scale from the spread of points\n",
        "        spread_A = np.std(A, axis=0)\n",
        "        spread_B = np.std(B, axis=0)\n",
        "        scale = np.mean(spread_B / (spread_A + 1e-6))\n",
        "        \n",
        "        # Apply simple similarity transform: scale + translation\n",
        "        A_centered = A - centroid_A\n",
        "        A_aligned = A_centered * scale + centroid_B\n",
        "        return A_aligned\n",
        "    \n",
        "    # Original Procrustes for same-sized point sets\n",
        "    centroid_A = A.mean(axis=0)\n",
        "    centroid_B = B.mean(axis=0)\n",
        "    A_c = A - centroid_A\n",
        "    B_c = B - centroid_B\n",
        "    # Compute covariance matrix and SVD\n",
        "    H = A_c.T @ B_c\n",
        "    U, S, Vt = np.linalg.svd(H)\n",
        "    R = Vt.T @ U.T\n",
        "    # Ensure a proper rotation (determinant = 1)\n",
        "    if np.linalg.det(R) < 0:\n",
        "        Vt[-1, :] *= -1\n",
        "        R = Vt.T @ U.T\n",
        "    var_A = np.sum(A_c ** 2)\n",
        "    # Scale\n",
        "    scale = np.sum(S) / var_A if var_A > 0 else 1.0\n",
        "    # Apply transformation\n",
        "    A_aligned = (A_c @ R) * scale + centroid_B\n",
        "    return A_aligned\n",
        "\n",
        "def align_points(or_points: np.ndarray, he_points: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Align Orion core centres to H&E using either CPD (if available) or Procrustes alignment.\n",
        "    Returns the aligned Orion points in H&E space.\n",
        "    \"\"\"\n",
        "    if CPD_AVAILABLE and len(or_points) > 0 and len(he_points) > 0:\n",
        "        reg = AffineRegistration(X=he_points, Y=or_points)\n",
        "        TY, (s, R, t) = reg.register()\n",
        "        return TY\n",
        "    else:\n",
        "        return procrustes_align(or_points, he_points)\n",
        "\n",
        "def pair_points(\n",
        "    or_points: np.ndarray,\n",
        "    he_points: np.ndarray,\n",
        "    max_dist_factor: float = 3.0\n",
        ") -> List[Tuple[int, int]]:\n",
        "    \"\"\"\n",
        "    Pair points between two sets using the Hungarian algorithm.\n",
        "\n",
        "    A distance matrix of size (n_orion, n_he) is computed.\n",
        "    Pairs with distances greater than `max_dist_factor * median_spacing` are rejected.\n",
        "\n",
        "    Returns a list of tuples (i, j) indexing the matched Orion and H&E points.\n",
        "    \"\"\"\n",
        "    if or_points.size == 0 or he_points.size == 0:\n",
        "        return []\n",
        "    # Compute distance matrix\n",
        "    diff = or_points[:, None, :] - he_points[None, :, :]\n",
        "    dists = np.linalg.norm(diff, axis=2)\n",
        "    # Estimate typical spacing from he_points (median nearest neighbour distance)\n",
        "    from scipy.spatial import cKDTree\n",
        "    kd = cKDTree(he_points)\n",
        "    nn_dists, _ = kd.query(he_points, k=2)\n",
        "    spacing = np.median(nn_dists[:, 1]) if len(he_points) >= 2 else np.median(dists)\n",
        "    # Solve assignment\n",
        "    row_ind, col_ind = linear_sum_assignment(dists)\n",
        "    pairs = []\n",
        "    for r, c in zip(row_ind, col_ind):\n",
        "        if dists[r, c] <= max_dist_factor * spacing:\n",
        "            pairs.append((r, c))\n",
        "    return pairs\n",
        "\n",
        "def extract_region(\n",
        "    slide_path: str,\n",
        "    center: Tuple[int, int],\n",
        "    patch_size: int = 2048\n",
        ") -> Optional[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Extract a square patch of size `patch_size`×`patch_size` centred at `center` from the full resolution slide.\n",
        "    Assumes `tifffile` is available.  Returns None on failure.\n",
        "    \"\"\"\n",
        "    if not TIFF_AVAILABLE:\n",
        "        raise RuntimeError('tifffile is required for full‑resolution extraction')\n",
        "    half = patch_size // 2\n",
        "    cx, cy = center\n",
        "    try:\n",
        "        with tifffile.TiffFile(slide_path) as tif:\n",
        "            series = tif.series[0]\n",
        "            full_shape = series.shape[-2:]\n",
        "            h_f, w_f = full_shape\n",
        "            left = max(0, cx - half)\n",
        "            top = max(0, cy - half)\n",
        "            right = min(w_f, cx + half)\n",
        "            bottom = min(h_f, cy + half)\n",
        "            arr = series.asarray(region=(top, left, bottom, right))\n",
        "            if arr.ndim == 3 and arr.shape[0] < arr.shape[-1]:\n",
        "                arr = np.transpose(arr, (1, 2, 0))\n",
        "            return arr\n",
        "    except Exception as e:\n",
        "        print(f'Failed to extract region: {e}')\n",
        "        return None\n",
        "\n",
        "def create_overlay(he_patch: np.ndarray, or_patch: np.ndarray, max_side: int = 512) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Create a quick‑look overlay by mapping the H&E patch to the red channel and the Orion patch to the green channel.\n",
        "    Optionally downsample to `max_side` for storage efficiency.\n",
        "    \"\"\"\n",
        "    def to_gray(p):\n",
        "        if p.ndim == 3:\n",
        "            if p.shape[2] >= 1:\n",
        "                return cv2.normalize(p[..., 0], None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "        return cv2.normalize(p, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "    he_gray = to_gray(he_patch)\n",
        "    or_gray = to_gray(or_patch)\n",
        "    if he_gray.shape != or_gray.shape:\n",
        "        or_gray = cv2.resize(or_gray, he_gray.shape[::-1], interpolation=cv2.INTER_AREA)\n",
        "    overlay = np.zeros((*he_gray.shape, 3), dtype=np.uint8)\n",
        "    overlay[..., 2] = he_gray\n",
        "    overlay[..., 1] = or_gray\n",
        "    h, w = overlay.shape[:2]\n",
        "    if max(h, w) > max_side:\n",
        "        scale = max_side / float(max(h, w))\n",
        "        overlay = cv2.resize(overlay, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_AREA)\n",
        "    return overlay\n",
        "\n",
        "def process_slides(\n",
        "    he_path: str,\n",
        "    orion_path: str,\n",
        "    out_dir: str = 'paired_dataset',\n",
        "    thumb_size: int = 4096,\n",
        "    patch_size: int = 2048,\n",
        "    min_core_area: int = 100,\n",
        "    circularity_thresh: float = 0.1,\n",
        "    max_dist_factor: float = 3.0\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Run the full pipeline on the provided H&E and Orion slides.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    he_path, orion_path : str\n",
        "        Paths to the OME‑TIFF images.\n",
        "    out_dir : str\n",
        "        Root output directory where patches, overlays and CSV will be written.\n",
        "    thumb_size : int\n",
        "        Target size of the thumbnail used for detection.\n",
        "    patch_size : int\n",
        "        Side length (in pixels) of the full‑resolution extracted patches.\n",
        "    min_core_area : int\n",
        "        Minimum area of detected contours on the thumbnail.\n",
        "    circularity_thresh : float\n",
        "        Minimum circularity metric for contour acceptance.\n",
        "    max_dist_factor : float\n",
        "        Maximum allowed distance (relative to median core spacing) for pairing.\n",
        "    \"\"\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    patch_dir = os.path.join(out_dir, 'patches')\n",
        "    overlay_dir = os.path.join(out_dir, 'qc_overlays')\n",
        "    os.makedirs(patch_dir, exist_ok=True)\n",
        "    os.makedirs(overlay_dir, exist_ok=True)\n",
        "    print('Loading H&E thumbnail...')\n",
        "    he_thumb, he_full_shape = load_thumbnail(he_path, thumb_size=thumb_size, use_channel0=False)\n",
        "    print('Detecting H&E cores...')\n",
        "    he_cores = detect_cores(he_thumb, brightfield=True, min_area=min_core_area, circularity_thresh=circularity_thresh)\n",
        "    he_centres = scale_centres(he_cores, he_thumb.shape, he_full_shape)\n",
        "    print(f'Detected {len(he_centres)} H&E cores')\n",
        "    print('Loading Orion thumbnail...')\n",
        "    or_thumb, or_full_shape = load_thumbnail(orion_path, thumb_size=thumb_size, use_channel0=True)\n",
        "    print('Detecting Orion cores...')\n",
        "    or_cores = detect_cores(or_thumb, brightfield=False, min_area=min_core_area, circularity_thresh=circularity_thresh)\n",
        "    or_centres = scale_centres(or_cores, or_thumb.shape, or_full_shape)\n",
        "    print(f'Detected {len(or_centres)} Orion cores')\n",
        "    he_centres_arr = np.array(he_centres)\n",
        "    or_centres_arr = np.array(or_centres)\n",
        "    if len(he_centres_arr) > 0 and len(or_centres_arr) > 0:\n",
        "        print('Estimating global transformation...')\n",
        "        or_aligned = align_points(or_centres_arr, he_centres_arr)\n",
        "    else:\n",
        "        or_aligned = or_centres_arr\n",
        "    print('Pairing cores...')\n",
        "    pairs = pair_points(or_aligned, he_centres_arr, max_dist_factor=max_dist_factor)\n",
        "    print(f'Found {len(pairs)} pairs')\n",
        "    csv_rows = []\n",
        "    for idx, (or_idx, he_idx) in enumerate(pairs, 1):\n",
        "        he_center = he_centres_arr[he_idx]\n",
        "        or_center = or_centres_arr[or_idx]\n",
        "        he_patch = extract_region(he_path, tuple(he_center), patch_size=patch_size)\n",
        "        or_patch = extract_region(orion_path, tuple(or_center), patch_size=patch_size)\n",
        "        if he_patch is None or or_patch is None:\n",
        "            print(f'Skipping pair {idx} due to extraction failure')\n",
        "            continue\n",
        "        he_filename = f'core_{idx:03d}_he.tiff'\n",
        "        or_filename = f'core_{idx:03d}_orion.tiff'\n",
        "        he_patch_path = os.path.join(patch_dir, he_filename)\n",
        "        or_patch_path = os.path.join(patch_dir, or_filename)\n",
        "        tifffile.imwrite(he_patch_path, he_patch, compression='zlib')\n",
        "        tifffile.imwrite(or_patch_path, or_patch, compression='zlib')\n",
        "        overlay = create_overlay(he_patch, or_patch, max_side=512)\n",
        "        overlay_name = f'core_{idx:03d}_overlay.png'\n",
        "        overlay_path = os.path.join(overlay_dir, overlay_name)\n",
        "        cv2.imwrite(overlay_path, overlay)\n",
        "        csv_rows.append([idx, he_patch_path, or_patch_path, overlay_path, float(np.linalg.norm(or_aligned[or_idx] - he_centres_arr[he_idx]))])\n",
        "        print(f'Processed pair {idx}/{len(pairs)}')\n",
        "    csv_path = os.path.join(out_dir, 'paired_core_info.csv')\n",
        "    with open(csv_path, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['pair_index', 'he_patch_path', 'orion_patch_path', 'overlay_path', 'pair_distance'])\n",
        "        writer.writerows(csv_rows)\n",
        "    print(f'Wrote summary CSV to {csv_path}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the pipeline\n",
        "\n",
        "To run the pipeline, provide the paths to your H&E and Orion OME‑TIFF files and specify an output directory.  Adjust `thumb_size`, `patch_size`, `min_core_area`, `circularity_thresh` and `max_dist_factor` as needed.  Note that full‑resolution extraction requires `tifffile`.\n",
        "\n",
        "```python\n",
        "# Example usage (uncomment to run)\n",
        "# he_slide = 'data/raw/TA118-HEraw.ome.tiff'\n",
        "# orion_slide = 'data/raw/TA118-Orionraw.ome.tiff'\n",
        "# process_slides(he_slide, orion_slide, out_dir='paired_dataset')\n",
        "```\n",
        "\n",
        "The resulting directory `paired_dataset` will contain:\n",
        "* `patches/` – paired patches named `core_XXX_he.tiff` and `core_XXX_orion.tiff`.\n",
        "* `qc_overlays/` – PNG overlays for quick visual quality control.\n",
        "* `paired_core_info.csv` – a summary of all core pairs with their file paths and pairing distances.\n",
        "\n",
        "Once cores are paired and patches are extracted, you can perform precise multi‑modal registration on each pair using tools such as VALIS, Elastix or the RMI/MIND based methods discussed in the accompanying analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "he_slide = 'data/raw/TA118-HEraw.ome.tiff'\n",
        "orion_slide = 'data/raw/TA118-Orionraw.ome.tiff'\n",
        "process_slides(he_slide, orion_slide, out_dir='paired_dataset_aug18')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
