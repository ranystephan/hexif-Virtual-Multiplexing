{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# SPACEc Analysis of CODEX Data: ccRCC and ccOC TMAs\n",
    "\n",
    "This notebook provides a comprehensive workflow for analyzing CODEX (CO-Detection by indeXing) data from clear cell renal cell carcinoma (ccRCC) and clear cell ovarian carcinoma (ccOC) tissue microarrays (TMAs) using the SPACEc (Spatial Protein Analysis with Clustering and Enrichment) library.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The workflow includes:\n",
    "1. **Data Loading and Inspection**: Load CODEX data and visualize tissue images\n",
    "2. **Cell Segmentation**: Segment individual cells using deep learning models\n",
    "3. **Preprocessing**: Normalize data, filter cells, and remove noise\n",
    "4. **Clustering & Annotation**: Identify cell types through unsupervised clustering\n",
    "5. **Expression Matrix Extraction**: Generate protein × cell expression matrices\n",
    "6. **Spatial Analysis**: Analyze cell locations and spatial relationships\n",
    "7. **Visualization**: Create comprehensive visualizations including thumbnails\n",
    "\n",
    "## Data Structure\n",
    "\n",
    "Our data includes:\n",
    "- **ccRCC TMAs**: Clear cell renal cell carcinoma tissue microarrays\n",
    "- **ccOC TMAs**: Clear cell ovarian carcinoma tissue microarrays  \n",
    "- **Protein markers**: 53 markers including immune, epithelial, and stromal markers\n",
    "- **Spatial coordinates**: Cell locations within tissue sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181a83ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SPACEc and required libraries\n",
    "import spacec as sp\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting parameters\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rc('axes', grid=False)\n",
    "sc.settings.set_figure_params(dpi=80, facecolor='white')\n",
    "\n",
    "# Set default colormap\n",
    "plt.rcParams[\"image.cmap\"] = 'viridis'\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(\"✓ Plotting parameters configured\")\n",
    "\n",
    "# Check for GPU availability (optional for faster segmentation)\n",
    "try:\n",
    "    sp.hf.check_for_gpu()\n",
    "except:\n",
    "    print(\"GPU not available, will use CPU for segmentation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "root_path = Path.cwd().parent.parent  # hexif root directory\n",
    "data_path = root_path / 'data' / 'nomad_data' / 'CODEX'\n",
    "output_dir = root_path / 'output' / 'spacec_analysis'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Print paths for verification\n",
    "print(f\"Root path: {root_path}\")\n",
    "print(f\"Data path: {data_path}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Data path exists: {data_path.exists()}\")\n",
    "\n",
    "# Get list of TMA directories\n",
    "tma_dirs = [d for d in data_path.iterdir() if d.is_dir() and 'TMA' in d.name]\n",
    "print(f\"\\nFound {len(tma_dirs)} TMA directories:\")\n",
    "for tma_dir in sorted(tma_dirs):\n",
    "    print(f\"  - {tma_dir.name}\")\n",
    "\n",
    "# Load marker list\n",
    "marker_file = data_path / 'MarkerList.txt'\n",
    "if marker_file.exists():\n",
    "    with open(marker_file, 'r') as f:\n",
    "        markers = [line.strip() for line in f.readlines()]\n",
    "    print(f\"\\nLoaded {len(markers)} markers from MarkerList.txt\")\n",
    "    print(f\"Markers: {', '.join(markers[:10])}{'...' if len(markers) > 10 else ''}\")\n",
    "else:\n",
    "    print(\"MarkerList.txt not found!\")\n",
    "\n",
    "# Load TMA mapping files\n",
    "mapping_files = list(data_path.glob('*TMA*map*.xlsx'))\n",
    "print(f\"\\nFound {len(mapping_files)} TMA mapping files:\")\n",
    "for mapping_file in mapping_files:\n",
    "    print(f\"  - {mapping_file.name}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Data Inspection and Visualization\n",
    "\n",
    "Let's examine the structure of our CODEX data and visualize sample tissues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_tma_structure(tma_dir):\n",
    "    \"\"\"Inspect the structure of a TMA directory\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Inspecting: {tma_dir.name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Check for bestFocus directory\n",
    "    bestfocus_dir = tma_dir / 'bestFocus'\n",
    "    if bestfocus_dir.exists():\n",
    "        tif_files = list(bestfocus_dir.glob('*.tif'))\n",
    "        print(f\"bestFocus directory: {len(tif_files)} TIF files\")\n",
    "        if tif_files:\n",
    "            print(f\"  Sample files: {', '.join([f.name for f in tif_files[:3]])}...\")\n",
    "    \n",
    "    # Check for Scan1 directory\n",
    "    scan1_dir = tma_dir / 'Scan1'\n",
    "    if scan1_dir.exists():\n",
    "        qptiff_files = list(scan1_dir.glob('*.qptiff'))\n",
    "        xpd_files = list(scan1_dir.glob('*.xpd'))\n",
    "        marker_files = list(scan1_dir.glob('MarkerList.txt'))\n",
    "        \n",
    "        print(f\"Scan1 directory contents:\")\n",
    "        print(f\"  - QPTIFF files: {len(qptiff_files)}\")\n",
    "        print(f\"  - XPD files: {len(xpd_files)}\")\n",
    "        print(f\"  - Marker files: {len(marker_files)}\")\n",
    "        \n",
    "        # Check .temp directory\n",
    "        temp_dir = scan1_dir / '.temp'\n",
    "        if temp_dir.exists():\n",
    "            temp_files = list(temp_dir.glob('*'))\n",
    "            print(f\"  - .temp directory: {len(temp_files)} files\")\n",
    "    \n",
    "    return bestfocus_dir, scan1_dir\n",
    "\n",
    "# Inspect all TMA directories\n",
    "tma_info = {}\n",
    "for tma_dir in sorted(tma_dirs):\n",
    "    bestfocus_dir, scan1_dir = inspect_tma_structure(tma_dir)\n",
    "    tma_info[tma_dir.name] = {\n",
    "        'path': tma_dir,\n",
    "        'bestfocus': bestfocus_dir,\n",
    "        'scan1': scan1_dir\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae55888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thumbnail_grid(tif_files, max_images=12, figsize=(15, 10)):\n",
    "    \"\"\"Create a grid of thumbnail images from TIF files\"\"\"\n",
    "    \n",
    "    # Limit the number of images\n",
    "    tif_files = tif_files[:max_images]\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    n_images = len(tif_files)\n",
    "    n_cols = min(4, n_images)\n",
    "    n_rows = (n_images + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    \n",
    "    # Ensure axes is always a 2D array\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axes = [[axes]]\n",
    "    elif n_rows == 1:\n",
    "        axes = [axes]\n",
    "    elif n_cols == 1:\n",
    "        axes = [[ax] for ax in axes]\n",
    "    \n",
    "    for i, tif_file in enumerate(tif_files):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        ax = axes[row][col]\n",
    "        \n",
    "        try:\n",
    "            # Try to load TIF file - use PIL as fallback\n",
    "            try:\n",
    "                import tifffile\n",
    "                img = tifffile.imread(str(tif_file))\n",
    "            except:\n",
    "                from PIL import Image\n",
    "                img = np.array(Image.open(str(tif_file)))\n",
    "            \n",
    "            # Handle different array dimensions\n",
    "            if img.ndim == 3:\n",
    "                # Use first channel, normalize for display\n",
    "                img_display = img[0] if img.shape[0] < img.shape[2] else img[:, :, 0]\n",
    "            else:\n",
    "                img_display = img\n",
    "                \n",
    "            # Normalize for display\n",
    "            if img_display.max() > img_display.min():\n",
    "                img_display = (img_display - img_display.min()) / (img_display.max() - img_display.min())\n",
    "            \n",
    "            ax.imshow(img_display, cmap='gray')\n",
    "            ax.set_title(f\"{tif_file.name}\", fontsize=8)\n",
    "            ax.axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, f\"Error loading\\\\n{tif_file.name}\", \n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=8)\n",
    "            ax.set_title(f\"{tif_file.name}\", fontsize=8)\n",
    "            ax.axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_images, n_rows * n_cols):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        ax = axes[row][col]\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Create thumbnail grids for each TMA\n",
    "print(\"\\\\nCreating thumbnail visualizations...\")\n",
    "for tma_name, info in tma_info.items():\n",
    "    if info['bestfocus'].exists():\n",
    "        tif_files = list(info['bestfocus'].glob('*.tif'))\n",
    "        if tif_files:\n",
    "            print(f\"\\\\nThumbnails for {tma_name}:\")\n",
    "            fig = create_thumbnail_grid(tif_files, max_images=12)\n",
    "            plt.suptitle(f\"{tma_name} - bestFocus TIF Files\", fontsize=14, y=0.98)\n",
    "            plt.show()\n",
    "            \n",
    "            # Save thumbnail grid\n",
    "            thumbnail_path = output_dir / f\"{tma_name}_thumbnails.png\"\n",
    "            fig.savefig(thumbnail_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved thumbnail grid to: {thumbnail_path}\")\n",
    "            plt.close()\n",
    "    else:\n",
    "        print(f\"No bestFocus directory found for {tma_name}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Cell Segmentation\n",
    "\n",
    "Cell segmentation is crucial for identifying individual cells and extracting single-cell data. We'll use deep learning-based segmentation methods (Mesmer or Cellpose) to identify cell boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a representative TMA for detailed analysis\n",
    "# For this example, we'll use the first ccRCC TMA\n",
    "selected_tma = None\n",
    "for tma_name, info in tma_info.items():\n",
    "    if 'ccRCC' in tma_name and info['bestfocus'].exists():\n",
    "        selected_tma = info\n",
    "        selected_tma_name = tma_name\n",
    "        break\n",
    "\n",
    "if selected_tma is None:\n",
    "    print(\"No suitable TMA found for analysis\")\n",
    "else:\n",
    "    print(f\"Selected TMA for analysis: {selected_tma_name}\")\n",
    "    \n",
    "    # Get list of TIF files in bestFocus directory\n",
    "    tif_files = list(selected_tma['bestfocus'].glob('*.tif'))\n",
    "    print(f\"Found {len(tif_files)} TIF files\")\n",
    "    \n",
    "    # Create a channel names file for the selected TMA\n",
    "    channel_file = output_dir / f\"{selected_tma_name}_channelnames.txt\"\n",
    "    with open(channel_file, 'w') as f:\n",
    "        for marker in markers:\n",
    "            f.write(f\"{marker}\\\\n\")\n",
    "    \n",
    "    print(f\"Created channel names file: {channel_file}\")\n",
    "    \n",
    "    # Select first few TIF files for segmentation (to manage processing time)\n",
    "    selected_tifs = tif_files[:3]  # Process first 3 TIF files\n",
    "    print(f\"Selected {len(selected_tifs)} TIF files for segmentation:\")\n",
    "    for tif in selected_tifs:\n",
    "        print(f\"  - {tif.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e96cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize segmentation channels before segmentation\n",
    "if selected_tma and len(selected_tifs) > 0:\n",
    "    sample_tif = selected_tifs[0]\n",
    "    print(f\"\\\\nVisualizing segmentation channels for {sample_tif.name}...\")\n",
    "    \n",
    "    # Define membrane markers for segmentation\n",
    "    membrane_markers = ['CD45', 'CD31', 'panCK']  # Adjust based on your markers\n",
    "    available_membrane = [m for m in membrane_markers if m in markers]\n",
    "    \n",
    "    if available_membrane:\n",
    "        print(f\"Available membrane markers: {available_membrane}\")\n",
    "        \n",
    "        # Visualize segmentation channels\n",
    "        sp.pl.segmentation_ch(\n",
    "            file_name=str(sample_tif),\n",
    "            channel_file=str(channel_file),\n",
    "            output_dir=str(output_dir),\n",
    "            extra_seg_ch_list=available_membrane,\n",
    "            nuclei_channel='DAPI',\n",
    "            input_format='CODEX'\n",
    "        )\n",
    "    else:\n",
    "        print(\"No suitable membrane markers found, using nuclei only\")\n",
    "        \n",
    "        # Visualize nuclei channel only\n",
    "        sp.pl.segmentation_ch(\n",
    "            file_name=str(sample_tif),\n",
    "            channel_file=str(channel_file),\n",
    "            output_dir=str(output_dir),\n",
    "            nuclei_channel='DAPI',\n",
    "            input_format='CODEX'\n",
    "        )\n",
    "        \n",
    "print(\"\\\\nSegmentation channel visualization complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72acd7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cell segmentation using Mesmer\n",
    "if selected_tma and len(selected_tifs) > 0:\n",
    "    print(\"\\\\nStarting cell segmentation...\")\n",
    "    \n",
    "    # Store segmentation results\n",
    "    segmentation_results = {}\n",
    "    \n",
    "    for i, tif_file in enumerate(selected_tifs):\n",
    "        print(f\"\\\\nProcessing {tif_file.name} ({i+1}/{len(selected_tifs)})...\")\n",
    "        \n",
    "        # Define output filename\n",
    "        output_name = f\"{selected_tma_name}_{tif_file.stem}\"\n",
    "        \n",
    "        # Perform segmentation\n",
    "        try:\n",
    "            seg_output = sp.tl.cell_segmentation(\n",
    "                file_name=str(tif_file),\n",
    "                channel_file=str(channel_file),\n",
    "                output_dir=str(output_dir),\n",
    "                seg_method='mesmer',  # Use Mesmer for segmentation\n",
    "                nuclei_channel='DAPI',\n",
    "                output_fname=output_name,\n",
    "                membrane_channel_list=available_membrane if available_membrane else None,\n",
    "                compartment='whole-cell',  # Segment whole cells\n",
    "                input_format='CODEX',\n",
    "                resize_factor=1,  # Adjust if images are too large\n",
    "                size_cutoff=0  # Minimum cell size filter\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            segmentation_results[tif_file.name] = {\n",
    "                'seg_output': seg_output,\n",
    "                'output_name': output_name\n",
    "            }\n",
    "            \n",
    "            print(f\"✓ Segmentation completed for {tif_file.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing {tif_file.name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\\\nSegmentation completed for {len(segmentation_results)} files\")\n",
    "else:\n",
    "    print(\"No files selected for segmentation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0325892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize segmentation results\n",
    "if segmentation_results:\n",
    "    print(\"\\\\nVisualizing segmentation results...\")\n",
    "    \n",
    "    # Create overlay data for visualization\n",
    "    overlay_data = {}\n",
    "    \n",
    "    for tif_name, seg_info in segmentation_results.items():\n",
    "        print(f\"\\\\nCreating overlay for {tif_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Create overlay visualization\n",
    "            overlay_data_single, rgb_images = sp.pl.show_masks(\n",
    "                seg_output=seg_info['seg_output'],\n",
    "                nucleus_channel='DAPI',\n",
    "                additional_channels=available_membrane if available_membrane else ['CD45'],\n",
    "                show_subsample=True,\n",
    "                n=4,  # Number of subsamples\n",
    "                tilesize=400,  # Size of tiles\n",
    "                rand_seed=42\n",
    "            )\n",
    "            \n",
    "            # Store overlay data\n",
    "            overlay_data[tif_name] = overlay_data_single\n",
    "            \n",
    "            print(f\"✓ Overlay created for {tif_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error creating overlay for {tif_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Save segmentation results\n",
    "    print(\"\\\\nSaving segmentation results...\")\n",
    "    for tif_name, seg_info in segmentation_results.items():\n",
    "        try:\n",
    "            # Save segmentation output\n",
    "            seg_pickle_path = output_dir / f\"{seg_info['output_name']}_segmentation.pickle\"\n",
    "            with open(seg_pickle_path, 'wb') as f:\n",
    "                pickle.dump(seg_info['seg_output'], f)\n",
    "            \n",
    "            # Save overlay data if available\n",
    "            if tif_name in overlay_data:\n",
    "                overlay_pickle_path = output_dir / f\"{seg_info['output_name']}_overlay.pickle\"\n",
    "                with open(overlay_pickle_path, 'wb') as f:\n",
    "                    pickle.dump(overlay_data[tif_name], f)\n",
    "            \n",
    "            print(f\"✓ Saved results for {tif_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error saving results for {tif_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\\\nSegmentation results saved successfully!\")\n",
    "else:\n",
    "    print(\"No segmentation results to visualize\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "In this section, we'll preprocess the segmented single-cell data by:\n",
    "1. Reading segmentation results\n",
    "2. Filtering cells by size and nuclear intensity\n",
    "3. Normalizing protein expression data\n",
    "4. Removing noisy cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b52610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read segmentation results and create dataframe\n",
    "if segmentation_results:\n",
    "    print(\"Reading segmentation results...\")\n",
    "    \n",
    "    # Collect CSV files from segmentation\n",
    "    csv_files = []\n",
    "    region_names = []\n",
    "    meta_info = []\n",
    "    \n",
    "    for tif_name, seg_info in segmentation_results.items():\n",
    "        csv_file = output_dir / f\"{seg_info['output_name']}_mesmer_result.csv\"\n",
    "        if csv_file.exists():\n",
    "            csv_files.append(str(csv_file))\n",
    "            region_names.append(tif_name.replace('.tif', ''))\n",
    "            \n",
    "            # Extract metadata from TIF filename\n",
    "            if 'ccRCC' in tif_name:\n",
    "                meta_info.append('ccRCC')\n",
    "            elif 'ccOC' in tif_name:\n",
    "                meta_info.append('ccOC')\n",
    "            else:\n",
    "                meta_info.append('unknown')\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files for processing\")\n",
    "    \n",
    "    # Read and concatenate segmentation results\n",
    "    if csv_files:\n",
    "        df_seg = sp.pp.read_segdf(\n",
    "            segfile_list=csv_files,\n",
    "            seg_method='mesmer',\n",
    "            region_list=region_names,\n",
    "            meta_list=meta_info\n",
    "        )\n",
    "        \n",
    "        print(f\"Combined dataframe shape: {df_seg.shape}\")\n",
    "        print(f\"Columns: {list(df_seg.columns)}\")\n",
    "        \n",
    "        # Display first few rows\n",
    "        print(\"\\\\nFirst 5 rows of segmentation data:\")\n",
    "        print(df_seg.head())\n",
    "        \n",
    "    else:\n",
    "        print(\"No CSV files found from segmentation\")\n",
    "        df_seg = None\n",
    "else:\n",
    "    print(\"No segmentation results available\")\n",
    "    df_seg = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter cells by DAPI intensity and area\n",
    "if df_seg is not None:\n",
    "    print(\"\\\\nFiltering cells by size and nuclear intensity...\")\n",
    "    \n",
    "    # Calculate filtering thresholds\n",
    "    one_percent_area = np.percentile(df_seg.area, 1)\n",
    "    one_percent_nuc = np.percentile(df_seg.DAPI, 1)\n",
    "    \n",
    "    print(f\"1% area threshold: {one_percent_area:.2f}\")\n",
    "    print(f\"1% DAPI threshold: {one_percent_nuc:.2f}\")\n",
    "    \n",
    "    # Apply filters\n",
    "    df_filt = sp.pp.filter_data(\n",
    "        df_seg,\n",
    "        nuc_thres=one_percent_nuc,\n",
    "        size_thres=one_percent_area,\n",
    "        nuc_marker=\"DAPI\",\n",
    "        cell_size=\"area\",\n",
    "        region_column=\"region_num\",\n",
    "        color_by=\"region_num\",\n",
    "        log_scale=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Filtered dataframe shape: {df_filt.shape}\")\n",
    "    print(f\"Removed {len(df_seg) - len(df_filt)} cells ({((len(df_seg) - len(df_filt))/len(df_seg)*100):.1f}%)\")\n",
    "    \n",
    "    # Show distribution of cells by region\n",
    "    print(\"\\\\nCells per region after filtering:\")\n",
    "    print(df_filt.groupby('unique_region').size())\n",
    "    \n",
    "else:\n",
    "    print(\"No segmentation data available for filtering\")\n",
    "    df_filt = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1711b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data per region\n",
    "if df_filt is not None:\n",
    "    print(\"\\\\nNormalizing protein expression data...\")\n",
    "    \n",
    "    # Identify columns to exclude from normalization\n",
    "    exclude_cols = ['eccentricity', 'perimeter', 'convex_area', 'axis_major_length', \n",
    "                   'axis_minor_length', 'label']\n",
    "    keep_cols = ['DAPI', 'x', 'y', 'area', 'region_num', 'unique_region', 'condition']\n",
    "    \n",
    "    # Normalize data per region\n",
    "    dfz = pd.DataFrame()\n",
    "    \n",
    "    for region in df_filt.unique_region.unique():\n",
    "        df_reg = df_filt[df_filt.unique_region == region]\n",
    "        \n",
    "        # Normalize using z-score\n",
    "        df_reg_norm = sp.pp.format(\n",
    "            data=df_reg,\n",
    "            list_out=exclude_cols,\n",
    "            list_keep=keep_cols,\n",
    "            method=\"zscore\"\n",
    "        )\n",
    "        \n",
    "        dfz = pd.concat([dfz, df_reg_norm], axis=0)\n",
    "    \n",
    "    print(f\"Normalized dataframe shape: {dfz.shape}\")\n",
    "    \n",
    "    # Find the column index for the last marker\n",
    "    protein_cols = [col for col in dfz.columns if col not in keep_cols]\n",
    "    if protein_cols:\n",
    "        last_marker_idx = dfz.columns.get_loc(protein_cols[-1])\n",
    "        print(f\"Last marker column index: {last_marker_idx}\")\n",
    "        print(f\"Protein markers: {protein_cols}\")\n",
    "    else:\n",
    "        print(\"No protein markers found\")\n",
    "        last_marker_idx = None\n",
    "        \n",
    "else:\n",
    "    print(\"No filtered data available for normalization\")\n",
    "    dfz = None\n",
    "    last_marker_idx = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcdd691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove noisy cells\n",
    "if dfz is not None and last_marker_idx is not None:\n",
    "    print(\"\\\\nRemoving noisy cells...\")\n",
    "    \n",
    "    # Determine noise thresholds per region\n",
    "    cutoff_dict = {}\n",
    "    for region in dfz.unique_region.unique():\n",
    "        df_reg = dfz[dfz.unique_region == region]\n",
    "        \n",
    "        # Visualize z-count threshold\n",
    "        print(f\"\\\\nAnalyzing noise threshold for region: {region}\")\n",
    "        sp.pl.zcount_thres(\n",
    "            dfz=df_reg,\n",
    "            col_num=last_marker_idx,\n",
    "            cut_off=0.01,  # Top 1% of cells\n",
    "            count_bin=50\n",
    "        )\n",
    "        \n",
    "        # Calculate thresholds (you might need to adjust these based on the plots)\n",
    "        # For demonstration, we'll use automated thresholds\n",
    "        end_idx = last_marker_idx + 1\n",
    "        protein_data = df_reg.iloc[:, :end_idx]\n",
    "        z_counts = (protein_data > 0).sum(axis=1)\n",
    "        z_sums = protein_data.sum(axis=1)\n",
    "        \n",
    "        z_count_threshold = np.percentile(z_counts, 99)  # Top 1%\n",
    "        z_sum_threshold = np.percentile(z_sums, 99)  # Top 1%\n",
    "        \n",
    "        cutoff_dict[region] = [z_count_threshold, z_sum_threshold]\n",
    "        print(f\"  Z-count threshold: {z_count_threshold:.1f}\")\n",
    "        print(f\"  Z-sum threshold: {z_sum_threshold:.1f}\")\n",
    "    \n",
    "    # Apply noise removal\n",
    "    df_nn = pd.DataFrame()\n",
    "    \n",
    "    for region in dfz.unique_region.unique():\n",
    "        df_reg = dfz[dfz.unique_region == region]\n",
    "        \n",
    "        df_reg_nn, removed_cells = sp.pp.remove_noise(\n",
    "            df=df_reg,\n",
    "            col_num=last_marker_idx,\n",
    "            z_count_thres=cutoff_dict[region][0],\n",
    "            z_sum_thres=cutoff_dict[region][1]\n",
    "        )\n",
    "        \n",
    "        print(f\"Region {region}: Removed {removed_cells} noisy cells\")\n",
    "        print(f\"  Remaining cells: {len(df_reg_nn)}\")\n",
    "        \n",
    "        df_nn = pd.concat([df_nn, df_reg_nn], axis=0)\n",
    "    \n",
    "    print(f\"\\\\nFinal dataframe shape after noise removal: {df_nn.shape}\")\n",
    "    print(f\"Total cells removed: {len(dfz) - len(df_nn)}\")\n",
    "    \n",
    "    # Save the preprocessed data\n",
    "    df_nn.to_csv(output_dir / \"preprocessed_data.csv\", index=False)\n",
    "    print(f\"Preprocessed data saved to: {output_dir / 'preprocessed_data.csv'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No normalized data available for noise removal\")\n",
    "    df_nn = None\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Clustering and Cell Type Annotation\n",
    "\n",
    "Now we'll use unsupervised clustering to identify cell types based on protein expression patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26648a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AnnData object for clustering\n",
    "if df_nn is not None:\n",
    "    print(\"Creating AnnData object for clustering...\")\n",
    "    \n",
    "    # Create AnnData object compatible with scanpy\n",
    "    adata = sp.hf.make_anndata(\n",
    "        df_nn=df_nn,\n",
    "        col_sum=last_marker_idx,\n",
    "        nonFuncAb_list=[]  # Remove non-functional antibodies if needed\n",
    "    )\n",
    "    \n",
    "    print(f\"AnnData object created: {adata}\")\n",
    "    print(f\"Number of cells: {adata.n_obs}\")\n",
    "    print(f\"Number of features: {adata.n_vars}\")\n",
    "    \n",
    "    # Save the AnnData object\n",
    "    adata.write_h5ad(output_dir / 'preprocessed_adata.h5ad')\n",
    "    print(f\"AnnData saved to: {output_dir / 'preprocessed_adata.h5ad'}\")\n",
    "    \n",
    "    # Display basic information\n",
    "    print(\"\\\\nMarkers for clustering:\")\n",
    "    print(list(adata.var_names)[:10], \"...\" if len(adata.var_names) > 10 else \"\")\n",
    "    print(f\"\\\\nConditions: {adata.obs['condition'].unique()}\")\n",
    "    print(f\"Regions: {adata.obs['unique_region'].unique()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No preprocessed data available for clustering\")\n",
    "    adata = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "if adata is not None:\n",
    "    print(\"\\\\nPerforming clustering...\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    clustering_random_seed = 42\n",
    "    \n",
    "    # Define markers for clustering (cancer-relevant markers)\n",
    "    clustering_markers = [\n",
    "        'CD3', 'CD4', 'CD8', 'CD20', 'CD68', 'CD45', 'panCK', 'Vimentin',\n",
    "        'CD31', 'CD34', 'FoxP3', 'Ki67', 'PD1', 'PDL1', 'CD11c', 'CD206',\n",
    "        'CD57', 'CD56', 'CD138', 'CD25', 'GZMB', 'HLA-DR', 'CD15', 'CD163',\n",
    "        'CD11b', 'CD90', 'aSMA', 'Ecadherin', 'CA9', 'HE4', 'VEGFR1', 'VEGFR2'\n",
    "    ]\n",
    "    \n",
    "    # Filter markers that are present in the data\n",
    "    available_markers = [m for m in clustering_markers if m in adata.var_names]\n",
    "    print(f\"Available markers for clustering: {len(available_markers)}\")\n",
    "    print(f\"Markers: {available_markers}\")\n",
    "    \n",
    "    # Perform clustering\n",
    "    adata = sp.tl.clustering(\n",
    "        adata,\n",
    "        clustering='leiden',\n",
    "        n_neighbors=15,\n",
    "        resolution=1.0,\n",
    "        reclustering=False,\n",
    "        marker_list=available_markers,\n",
    "        seed=clustering_random_seed\n",
    "    )\n",
    "    \n",
    "    # Visualize clustering results\n",
    "    print(\"\\\\nVisualizing clustering results...\")\n",
    "    \n",
    "    # UMAP visualization\n",
    "    sc.pl.umap(adata, color=['leiden_1', 'unique_region'], wspace=0.5, \n",
    "               save='_clustering_overview.pdf')\n",
    "    \n",
    "    # Create dot plot for marker expression\n",
    "    sc.pl.dotplot(adata, available_markers[:20], 'leiden_1', dendrogram=True,\n",
    "                  save='_marker_expression.pdf')\n",
    "    \n",
    "    print(f\"Number of clusters identified: {len(adata.obs['leiden_1'].unique())}\")\n",
    "    print(f\"Cluster distribution: {adata.obs['leiden_1'].value_counts().sort_index()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No AnnData object available for clustering\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Expression Matrix and Cell Location Extraction\n",
    "\n",
    "Now we'll extract the protein × cell expression matrix and cell spatial coordinates as requested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb73f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract protein x cell expression matrix\n",
    "if adata is not None:\n",
    "    print(\"Extracting protein × cell expression matrix...\")\n",
    "    \n",
    "    # Get expression matrix (cells × proteins)\n",
    "    expression_matrix = adata.X\n",
    "    protein_names = adata.var_names.tolist()\n",
    "    cell_ids = adata.obs_names.tolist()\n",
    "    \n",
    "    print(f\"Expression matrix shape: {expression_matrix.shape}\")\n",
    "    print(f\"Number of cells: {len(cell_ids)}\")\n",
    "    print(f\"Number of proteins: {len(protein_names)}\")\n",
    "    \n",
    "    # Create DataFrame for easier handling\n",
    "    expression_df = pd.DataFrame(\n",
    "        expression_matrix,\n",
    "        index=cell_ids,\n",
    "        columns=protein_names\n",
    "    )\n",
    "    \n",
    "    # Add cell metadata\n",
    "    cell_metadata = adata.obs.copy()\n",
    "    cell_metadata['cell_id'] = cell_ids\n",
    "    \n",
    "    # Extract spatial coordinates\n",
    "    spatial_coords = cell_metadata[['x', 'y', 'cell_id', 'unique_region', 'condition']].copy()\n",
    "    \n",
    "    # Add cluster information if available\n",
    "    if 'leiden_1' in cell_metadata.columns:\n",
    "        spatial_coords['cluster'] = cell_metadata['leiden_1']\n",
    "        expression_df['cluster'] = cell_metadata['leiden_1']\n",
    "    \n",
    "    print(f\"\\\\nSpatial coordinates extracted for {len(spatial_coords)} cells\")\n",
    "    print(f\"Coordinate columns: {spatial_coords.columns.tolist()}\")\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\\\nExpression matrix summary:\")\n",
    "    print(expression_df.describe())\n",
    "    \n",
    "    print(\"\\\\nSpatial coordinates summary:\")\n",
    "    print(spatial_coords.describe())\n",
    "    \n",
    "else:\n",
    "    print(\"No clustered data available for extraction\")\n",
    "    expression_df = None\n",
    "    spatial_coords = None\n",
    "    cell_metadata = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save expression matrix and spatial coordinates\n",
    "if expression_df is not None and spatial_coords is not None:\n",
    "    print(\"\\\\nSaving expression matrix and spatial coordinates...\")\n",
    "    \n",
    "    # Save expression matrix\n",
    "    expression_file = output_dir / 'protein_x_cell_expression_matrix.csv'\n",
    "    expression_df.to_csv(expression_file)\n",
    "    print(f\"✓ Expression matrix saved to: {expression_file}\")\n",
    "    \n",
    "    # Save spatial coordinates\n",
    "    spatial_file = output_dir / 'cell_spatial_coordinates.csv'\n",
    "    spatial_coords.to_csv(spatial_file, index=False)\n",
    "    print(f\"✓ Spatial coordinates saved to: {spatial_file}\")\n",
    "    \n",
    "    # Save complete cell metadata\n",
    "    metadata_file = output_dir / 'cell_metadata.csv'\n",
    "    cell_metadata.to_csv(metadata_file)\n",
    "    print(f\"✓ Cell metadata saved to: {metadata_file}\")\n",
    "    \n",
    "    # Create a summary report\n",
    "    summary_file = output_dir / 'analysis_summary.txt'\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"SPACEc CODEX Analysis Summary\\\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\\\n\\\\n\")\n",
    "        f.write(f\"Analysis Date: {pd.Timestamp.now()}\\\\n\")\n",
    "        f.write(f\"Selected TMA: {selected_tma_name if 'selected_tma_name' in locals() else 'N/A'}\\\\n\")\n",
    "        f.write(f\"Number of TIF files processed: {len(selected_tifs) if 'selected_tifs' in locals() else 'N/A'}\\\\n\")\n",
    "        f.write(f\"Total cells analyzed: {len(expression_df)}\\\\n\")\n",
    "        f.write(f\"Number of proteins: {len(protein_names)}\\\\n\")\n",
    "        f.write(f\"Number of clusters: {len(adata.obs['leiden_1'].unique()) if 'leiden_1' in adata.obs else 'N/A'}\\\\n\")\n",
    "        f.write(f\"Regions: {', '.join(spatial_coords['unique_region'].unique())}\\\\n\")\n",
    "        f.write(f\"Conditions: {', '.join(spatial_coords['condition'].unique())}\\\\n\")\n",
    "        f.write(\"\\\\nFiles generated:\\\\n\")\n",
    "        f.write(f\"- {expression_file.name}\\\\n\")\n",
    "        f.write(f\"- {spatial_file.name}\\\\n\")\n",
    "        f.write(f\"- {metadata_file.name}\\\\n\")\n",
    "        f.write(f\"- preprocessed_adata.h5ad\\\\n\")\n",
    "        f.write(\"\\\\nProtein markers analyzed:\\\\n\")\n",
    "        for i, protein in enumerate(protein_names):\n",
    "            f.write(f\"{i+1:2d}. {protein}\\\\n\")\n",
    "    \n",
    "    print(f\"✓ Analysis summary saved to: {summary_file}\")\n",
    "    \n",
    "    # Display file sizes\n",
    "    print(\"\\\\nGenerated files:\")\n",
    "    for file_path in [expression_file, spatial_file, metadata_file, summary_file]:\n",
    "        if file_path.exists():\n",
    "            size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  {file_path.name}: {size_mb:.2f} MB\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data available to save\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Comprehensive Visualization\n",
    "\n",
    "Create publication-quality visualizations of the results including spatial plots, expression heatmaps, and cluster analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be302a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "if adata is not None and spatial_coords is not None:\n",
    "    print(\"Creating comprehensive visualizations...\")\n",
    "    \n",
    "    # 1. Spatial distribution of clusters\n",
    "    print(\"\\\\n1. Creating spatial cluster plots...\")\n",
    "    \n",
    "    # Spatial plot of clusters\n",
    "    sp.pl.catplot(\n",
    "        adata,\n",
    "        color=\"leiden_1\",\n",
    "        unique_region=\"condition\",\n",
    "        X='x', Y='y',\n",
    "        n_columns=2,\n",
    "        palette='tab20',\n",
    "        savefig=True,\n",
    "        output_fname=\"spatial_clusters\",\n",
    "        output_dir=str(output_dir),\n",
    "        figsize=15,\n",
    "        size=5\n",
    "    )\n",
    "    \n",
    "    # 2. Expression heatmap\n",
    "    print(\"\\\\n2. Creating expression heatmaps...\")\n",
    "    \n",
    "    # Select top markers for heatmap\n",
    "    top_markers = available_markers[:15] if len(available_markers) >= 15 else available_markers\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    cluster_means = []\n",
    "    cluster_labels = []\n",
    "    \n",
    "    for cluster in sorted(adata.obs['leiden_1'].unique()):\n",
    "        cluster_cells = adata[adata.obs['leiden_1'] == cluster]\n",
    "        cluster_mean = cluster_cells.to_df()[top_markers].mean()\n",
    "        cluster_means.append(cluster_mean)\n",
    "        cluster_labels.append(f\"Cluster {cluster}\")\n",
    "    \n",
    "    heatmap_data = pd.DataFrame(cluster_means, index=cluster_labels)\n",
    "    \n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='RdBu_r', center=0, fmt='.2f')\n",
    "    plt.title('Mean Protein Expression by Cluster')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'cluster_expression_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Individual marker spatial plots\n",
    "    print(\"\\\\n3. Creating individual marker spatial plots...\")\n",
    "    \n",
    "    # Create spatial plots for key markers\n",
    "    key_markers = ['CD3', 'CD20', 'CD68', 'panCK', 'Ki67']\n",
    "    key_markers = [m for m in key_markers if m in adata.var_names]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, marker in enumerate(key_markers[:6]):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Get expression values\n",
    "        expression_values = adata.to_df()[marker]\n",
    "        \n",
    "        # Create scatter plot\n",
    "        scatter = ax.scatter(spatial_coords['x'], spatial_coords['y'], \n",
    "                           c=expression_values, cmap='viridis', s=1, alpha=0.6)\n",
    "        \n",
    "        ax.set_title(f'{marker} Expression')\n",
    "        ax.set_xlabel('X coordinate')\n",
    "        ax.set_ylabel('Y coordinate')\n",
    "        plt.colorbar(scatter, ax=ax, shrink=0.8)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(key_markers), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'marker_spatial_expression.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\\\n✓ All visualizations created successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data available for visualization\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "### Analysis Summary\n",
    "\n",
    "This notebook provided a comprehensive workflow for analyzing CODEX data from ccRCC and ccOC tissue microarrays using SPACEc. The analysis included:\n",
    "\n",
    "1. **Data Loading and Inspection**: Explored TMA directory structure and visualized tissue thumbnails\n",
    "2. **Cell Segmentation**: Used deep learning (Mesmer) to identify individual cells\n",
    "3. **Preprocessing**: Applied filtering, normalization, and noise removal\n",
    "4. **Clustering**: Identified cell populations using unsupervised clustering\n",
    "5. **Expression Matrix Extraction**: Generated protein × cell expression matrices\n",
    "6. **Spatial Analysis**: Analyzed cell locations and spatial relationships\n",
    "7. **Visualization**: Created comprehensive visualizations including spatial plots and heatmaps\n",
    "\n",
    "### Output Files Generated\n",
    "\n",
    "- `protein_x_cell_expression_matrix.csv`: Main expression matrix (cells × proteins)\n",
    "- `cell_spatial_coordinates.csv`: Cell locations and metadata\n",
    "- `cell_metadata.csv`: Complete cell metadata including clusters\n",
    "- `preprocessed_adata.h5ad`: AnnData object for further analysis\n",
    "- `analysis_summary.txt`: Detailed analysis summary\n",
    "- Various visualization files (PNG/PDF format)\n",
    "\n",
    "### Key Results\n",
    "\n",
    "- **Protein Expression Matrix**: Contains normalized expression values for all detected proteins across all segmented cells\n",
    "- **Cell Locations**: Spatial coordinates (x, y) for each cell within tissue sections\n",
    "- **Cell Type Identification**: Unsupervised clustering identified distinct cell populations\n",
    "- **Spatial Relationships**: Visualizations showing cell distribution patterns within tissues\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Cell Type Annotation**: Manually annotate clusters based on marker expression patterns\n",
    "2. **Neighborhood Analysis**: Analyze cellular neighborhoods and spatial interactions\n",
    "3. **Differential Expression**: Compare protein expression between ccRCC and ccOC samples\n",
    "4. **Survival Analysis**: Correlate spatial features with clinical outcomes\n",
    "5. **Machine Learning**: Train models for automated cell type classification\n",
    "\n",
    "### Usage Instructions\n",
    "\n",
    "To use the generated expression matrix and spatial coordinates:\n",
    "\n",
    "```python\n",
    "# Load expression matrix\n",
    "expression_matrix = pd.read_csv('output/spacec_analysis/protein_x_cell_expression_matrix.csv', index_col=0)\n",
    "\n",
    "# Load spatial coordinates\n",
    "spatial_coords = pd.read_csv('output/spacec_analysis/cell_spatial_coordinates.csv')\n",
    "\n",
    "# Load complete metadata\n",
    "metadata = pd.read_csv('output/spacec_analysis/cell_metadata.csv', index_col=0)\n",
    "```\n",
    "\n",
    "### References\n",
    "\n",
    "- SPACEc Documentation: https://spacec.readthedocs.io/\n",
    "- CODEX Technology: https://www.akoyabio.com/codex/\n",
    "- Scanpy Documentation: https://scanpy.readthedocs.io/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
