{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Orion Inference Visualizer\n",
        "\n",
        "This notebook loads a trained ConvNeXt regression model (from `train_orion_patches.py`) and lets you interactively inspect:\n",
        "\n",
        "- H&E patch\n",
        "- Ground-truth per-marker heatmaps (robust-normalized to [0,1])\n",
        "- Predicted per-marker scalar (one value per marker per patch)\n",
        "\n",
        "Set `pairs_dir` and `checkpoint_path` below, then use the widgets to select a `basename` and `patch_index` (defaults to 40).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Config\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Change these as needed\n",
        "pairs_dir = Path('core_patches_npy')  # directory with *_HE.npy and *_ORION.npy\n",
        "# You can set either a checkpoint file OR a directory to search in\n",
        "checkpoint_path = None  # e.g., Path('runs/orion_seg/best_model.pth') or None\n",
        "model_dir = Path('runs/orion_seg')\n",
        "\n",
        "# Visualization defaults\n",
        "default_basename_idx = 0\n",
        "default_patch_index = 40\n",
        "patch_size = 224\n",
        "\n",
        "assert pairs_dir.exists(), f\"pairs_dir not found: {pairs_dir}\"\n",
        "\n",
        "# Resolve checkpoint\n",
        "resolved_ckpt = None\n",
        "if checkpoint_path is not None:\n",
        "    checkpoint_path = Path(checkpoint_path)\n",
        "    if checkpoint_path.exists() and checkpoint_path.is_file():\n",
        "        resolved_ckpt = checkpoint_path\n",
        "\n",
        "if resolved_ckpt is None and model_dir is not None and Path(model_dir).exists():\n",
        "    model_dir = Path(model_dir)\n",
        "    candidates = [\n",
        "        model_dir / 'checkpoint_epoch_10.pth',\n",
        "    ]\n",
        "    epoch_ckpts = sorted(model_dir.glob('checkpoint_epoch_*.pth'), key=lambda p: int(p.stem.split('_')[-1]), reverse=True)\n",
        "    candidates.extend(epoch_ckpts)\n",
        "    candidates.extend([\n",
        "        model_dir / 'best_model.pth',\n",
        "        model_dir / 'final_model.pth',\n",
        "    ])\n",
        "    for pth in candidates:\n",
        "        if pth.exists():\n",
        "            resolved_ckpt = pth\n",
        "            break\n",
        "\n",
        "assert resolved_ckpt is not None, f\"No checkpoint found. Set checkpoint_path or ensure model_dir has checkpoints. Tried model_dir={model_dir}\"\n",
        "print(f\"Using checkpoint: {resolved_ckpt}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from ipywidgets import interact, IntSlider, Dropdown, fixed\n",
        "\n",
        "# Reuse utility from training script\n",
        "def robust_norm01(a: np.ndarray, p1=1, p99=99, eps=1e-6) -> np.ndarray:\n",
        "    lo, hi = np.percentile(a, (p1, p99))\n",
        "    if hi <= lo:\n",
        "        return np.zeros_like(a, dtype=np.float32)\n",
        "    return np.clip((a - lo) / (hi - lo + eps), 0, 1).astype(np.float32)\n",
        "\n",
        "\n",
        "def discover_basenames(pairs_dir: Path):\n",
        "    out = []\n",
        "    for hef in sorted(pairs_dir.glob('core_*_HE.npy')):\n",
        "        base = hef.stem.replace('_HE', '')\n",
        "        if (pairs_dir / f\"{base}_ORION.npy\").exists():\n",
        "            out.append(base)\n",
        "    return out\n",
        "\n",
        "\n",
        "def list_grid_coords(H, W, ps=224, stride=112):\n",
        "    ys = [0] if H <= ps else list(range(0, max(1, H - ps) + 1, stride))\n",
        "    xs = [0] if W <= ps else list(range(0, max(1, W - ps) + 1, stride))\n",
        "    coords = [(y, x) for y in ys for x in xs]\n",
        "    return coords\n",
        "\n",
        "\n",
        "def load_pair(pairs_dir: Path, basename: str):\n",
        "    he = np.load(pairs_dir / f\"{basename}_HE.npy\", mmap_mode='r')\n",
        "    orion = np.load(pairs_dir / f\"{basename}_ORION.npy\", mmap_mode='r')\n",
        "    if orion.ndim == 3 and orion.shape[0] == 20:\n",
        "        orion = np.transpose(orion, (1, 2, 0))\n",
        "    return he, orion\n",
        "\n",
        "\n",
        "def get_patch(he: np.ndarray, orion: np.ndarray, y0: int, x0: int, ps: int):\n",
        "    he_crop = he[y0:y0+ps, x0:x0+ps, :]\n",
        "    or_crop = orion[y0:y0+ps, x0:x0+ps, :]\n",
        "    return he_crop, or_crop\n",
        "\n",
        "\n",
        "def scale_orion(or_crop: np.ndarray):\n",
        "    C = or_crop.shape[2]\n",
        "    or_scaled = np.zeros_like(or_crop, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        or_scaled[..., c] = robust_norm01(or_crop[..., c])\n",
        "    return or_scaled\n",
        "\n",
        "\n",
        "def to_tensor_image(he_crop: np.ndarray, ps: int):\n",
        "    # mimic eval transform from training: ToTensor + Resize + Normalize\n",
        "    tf_eval = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Resize(ps, antialias=True),\n",
        "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    he_img = (he_crop * 255).astype(np.uint8) if he_crop.dtype != np.uint8 else he_crop\n",
        "    return tf_eval(he_img)\n",
        "\n",
        "\n",
        "def prepare_target_vector(or_crop_scaled: np.ndarray):\n",
        "    # (H, W, C) -> (C,)\n",
        "    vec = or_crop_scaled.transpose(2, 0, 1).reshape(or_crop_scaled.shape[2], -1).mean(axis=1)\n",
        "    return torch.from_numpy(vec.astype(np.float32))\n",
        "\n",
        "# Fluorescence colormap utilities\n",
        "FLUOR_COLORS = [\n",
        "    (0.0, 0.5, 1.0),   # Blue\n",
        "    (0.0, 1.0, 0.0),   # Green\n",
        "    (1.0, 0.0, 0.0),   # Red\n",
        "    (1.0, 1.0, 0.0),   # Yellow\n",
        "    (1.0, 0.0, 1.0),   # Magenta\n",
        "    (0.0, 1.0, 1.0),   # Cyan\n",
        "    (1.0, 0.5, 0.0),   # Orange\n",
        "    (0.5, 0.0, 1.0),   # Purple\n",
        "    (0.0, 0.8, 0.4),   # Teal\n",
        "    (1.0, 0.2, 0.6),   # Pink\n",
        "    (0.6, 1.0, 0.2),   # Lime\n",
        "    (0.8, 0.4, 0.0),   # Brown\n",
        "    (0.4, 0.6, 1.0),   # Light Blue\n",
        "    (1.0, 0.8, 0.2),   # Gold\n",
        "    (0.6, 0.0, 0.6),   # Maroon\n",
        "    (0.0, 0.6, 0.8),   # Steel Blue\n",
        "    (0.8, 0.2, 0.4),   # Crimson\n",
        "    (0.2, 0.8, 0.6),   # Sea Green\n",
        "    (0.9, 0.6, 0.1),   # Dark Orange\n",
        "    (0.3, 0.3, 0.9),   # Royal Blue\n",
        "]\n",
        "\n",
        "def create_fluorescence_colormap(color):\n",
        "    colors = [(0, 0, 0), color]\n",
        "    return LinearSegmentedColormap.from_list('fluor', colors, N=256)\n",
        "\n",
        "FLUOR_CMAPS = [create_fluorescence_colormap(color) for color in FLUOR_COLORS]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model definitions (UNet and ConvNeXt heads)\n",
        "import torchvision.models as tvm\n",
        "\n",
        "class ConvNeXtHead(nn.Module):\n",
        "    def __init__(self, num_outputs: int = 20, backbone: str = 'convnext_small'):\n",
        "        super().__init__()\n",
        "        if backbone == 'convnext_tiny':\n",
        "            m = tvm.convnext_tiny(weights=tvm.ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
        "            in_f = m.classifier[2].in_features\n",
        "            m.classifier[2] = nn.Linear(in_f, num_outputs)\n",
        "        else:\n",
        "            m = tvm.convnext_small(weights=tvm.ConvNeXt_Small_Weights.IMAGENET1K_V1)\n",
        "            in_f = m.classifier[2].in_features\n",
        "            m.classifier[2] = nn.Linear(in_f, num_outputs)\n",
        "        self.backbone = m\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "# Import UNetSmall from training file if available\n",
        "UNetSmall = None\n",
        "try:\n",
        "    import sys\n",
        "    from pathlib import Path as _P\n",
        "    proj_root = _P('/Users/ranystephan/Library/Mobile Documents/com~apple~CloudDocs/Desktop/RA/ra_biomed/hexif')\n",
        "    if str(proj_root) not in sys.path:\n",
        "        sys.path.insert(0, str(proj_root))\n",
        "    from train_orion_patches import UNetSmall as _UNetSmall\n",
        "    UNetSmall = _UNetSmall\n",
        "except Exception as e:\n",
        "    UNetSmall = None\n",
        "\n",
        "\n",
        "def load_model_any(checkpoint_path: Path, device: torch.device = None):\n",
        "    \"\"\"\n",
        "    Load checkpoint and return (model, device, mode) where mode is one of:\n",
        "    - 'seg' for spatial output model (H,W per channel), e.g., UNetSmall\n",
        "    - 'reg' for regression vector model, e.g., ConvNeXtHead\n",
        "    \"\"\"\n",
        "    ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
        "    device = device or torch.device('cuda' if torch.cuda.is_available() else ('mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else 'cpu'))\n",
        "\n",
        "    state = ckpt.get('model', ckpt)\n",
        "    args = ckpt.get('args', {})\n",
        "\n",
        "    # Try detect architecture from keys\n",
        "    keys = list(state.keys())\n",
        "    if any(k.startswith(('enc1.', 'dec1.', 'up1.', 'down1.', 'out')) for k in keys) or any(k.startswith(('d1.', 'u1.', 'c1.')) for k in keys):\n",
        "        # Assume UNet-like spatial model\n",
        "        if UNetSmall is None:\n",
        "            raise RuntimeError('UNetSmall not importable; cannot render spatial predictions.')\n",
        "        model = UNetSmall(in_ch=3, out_ch=20, base=args.get('base_features', 32))\n",
        "        model.load_state_dict(state, strict=False)\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        return model, device, 'seg'\n",
        "    else:\n",
        "        # Fallback to ConvNeXt regression\n",
        "        backbone = 'convnext_small'\n",
        "        model = ConvNeXtHead(num_outputs=20, backbone=backbone)\n",
        "        missing, unexpected = model.load_state_dict(state, strict=False)\n",
        "        if missing or unexpected:\n",
        "            print('Loaded with missing/unexpected keys:', missing, unexpected)\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        return model, device, 'reg'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data lists and model\n",
        "basenames = discover_basenames(pairs_dir)\n",
        "assert len(basenames) > 0, f\"No paired cores found in {pairs_dir}\"\n",
        "\n",
        "model, device, model_mode = load_model_any(resolved_ckpt)\n",
        "print(f\"Model mode: {model_mode}\")\n",
        "\n",
        "# Cache shapes and grid coords for each basename\n",
        "shapes = {}\n",
        "grids = {}\n",
        "for b in basenames:\n",
        "    he, orion = load_pair(pairs_dir, b)\n",
        "    H, W = orion.shape[0], orion.shape[1]\n",
        "    shapes[b] = (H, W)\n",
        "    grids[b] = list_grid_coords(H, W, ps=patch_size, stride=patch_size//2)\n",
        "\n",
        "print(f\"Found {len(basenames)} cores. Example: {basenames[0]}\")\n",
        "print(f\"Grid patches for first core: {len(grids[basenames[0]])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference + visualization\n",
        "from math import ceil\n",
        "\n",
        "# If you have marker names, fill them here; else use indices 0..19\n",
        "marker_names = [f\"Marker {i}\" for i in range(20)]\n",
        "\n",
        "def infer_and_plot(basename: str, patch_index: int):\n",
        "    he, orion = load_pair(pairs_dir, basename)\n",
        "    coords = grids[basename]\n",
        "    if len(coords) == 0:\n",
        "        raise ValueError('No grid coordinates computed')\n",
        "    patch_index = max(0, min(patch_index, len(coords)-1))\n",
        "    y0, x0 = coords[patch_index]\n",
        "\n",
        "    he_crop, or_crop = get_patch(he, orion, y0, x0, patch_size)\n",
        "    or_scaled = scale_orion(or_crop)\n",
        "\n",
        "    # prepare input tensor\n",
        "    he_t = to_tensor_image(he_crop, patch_size).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(he_t)\n",
        "        if model_mode == 'seg':\n",
        "            # Expect (1, 20, H', W'); resize back to crop size if needed\n",
        "            pred_map = torch.sigmoid(out)\n",
        "            pred_map = pred_map.squeeze(0)  # (20, H', W')\n",
        "            if pred_map.shape[1:] != or_scaled.shape[:2]:\n",
        "                pred_map = F.interpolate(pred_map.unsqueeze(0), size=or_scaled.shape[:2], mode='bilinear', align_corners=False).squeeze(0)\n",
        "            pred_np = pred_map.detach().cpu().numpy()  # (20, H, W)\n",
        "        else:\n",
        "            # Regression vector per marker; repeat to form uniform maps\n",
        "            pred_vec = out.squeeze(0).detach().cpu().numpy()  # (20,)\n",
        "            pred_np = np.stack([np.full(or_scaled.shape[:2], np.clip(float(v), 0.0, 1.0), dtype=np.float32) for v in pred_vec], axis=0)\n",
        "\n",
        "    # display: For each marker, show [H&E | Predicted | Ground Truth]\n",
        "    num_markers = or_scaled.shape[2]\n",
        "    nrows = num_markers\n",
        "    ncols = 3\n",
        "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*3.0, nrows*3.0))\n",
        "    if nrows == 1:\n",
        "        axes = np.expand_dims(axes, 0)\n",
        "\n",
        "    for c in range(num_markers):\n",
        "        # Column 1: H&E\n",
        "        ax = axes[c, 0]\n",
        "        ax.imshow(he_crop)\n",
        "        title = marker_names[c] if c < len(marker_names) else f'Marker {c}'\n",
        "        ax.set_title(f\"{title} - H&E\", fontsize=9)\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Column 2: Predicted heatmap\n",
        "        ax = axes[c, 1]\n",
        "        cmap = FLUOR_CMAPS[c % len(FLUOR_CMAPS)]\n",
        "        ax.imshow(pred_np[c], cmap=cmap, vmin=0, vmax=1)\n",
        "        ax.set_title(\"Pred\", fontsize=9)\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Column 3: Ground truth heatmap for this marker\n",
        "        ax = axes[c, 2]\n",
        "        ax.imshow(or_scaled[..., c], cmap=cmap, vmin=0, vmax=1)\n",
        "        ax.set_title(\"GT\", fontsize=9)\n",
        "        ax.axis('off')\n",
        "\n",
        "    fig.suptitle(f\"{basename} | patch #{patch_index} at (y={y0}, x={x0})\", y=0.995)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Interactive controls\n",
        "basename_dd = Dropdown(options=basenames, value=basenames[default_basename_idx], description='basename')\n",
        "patch_slider = IntSlider(value=default_patch_index, min=0, max=max(0, len(grids[basenames[default_basename_idx]])-1), step=1, description='patch_index')\n",
        "\n",
        "def update_patch_slider(*args):\n",
        "    b = basename_dd.value\n",
        "    patch_slider.max = max(0, len(grids[b]) - 1)\n",
        "    patch_slider.value = min(patch_slider.value, patch_slider.max)\n",
        "\n",
        "basename_dd.observe(update_patch_slider, names='value')\n",
        "\n",
        "interact(infer_and_plot, basename=basename_dd, patch_index=patch_slider);\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
